{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## IMPORT, CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm, os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display as displayer\n",
    "import math\n",
    "\n",
    "class Config(object):\n",
    "    pass\n",
    "config = Config()\n",
    "config.batch_size = 128\n",
    "config.learning_rate = 0.1\n",
    "config.use_float16 = False\n",
    "# change data directory here\n",
    "config.data_path_dir = \"../../../datasets/cifar-10-batches-bin\"\n",
    "config.BN_epsilon = 1e-5\n",
    "config.BN_decay = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, batch_size = config.batch_size):\n",
    "        self.width = 32\n",
    "        self.height = 32\n",
    "        self.depth = 3\n",
    "        self.x_train, self.y_train = None, None\n",
    "        self.x_test, self.y_test = None, None\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = 10\n",
    "        \n",
    "    def readData(self, path, is_one_hot = True):\n",
    "        def combineChannels(t):\n",
    "            r = t[:, 1:1025].reshape((10000, 1024, 1))\n",
    "            g = t[:, 1025:2049].reshape((10000, 1024, 1))\n",
    "            b = t[:, 2049:].reshape((10000, 1024, 1))\n",
    "            return np.dstack((r, g, b)).reshape((10000, 32, 32, 3))\n",
    "            \n",
    "        \n",
    "        y_train, y_test = None, None\n",
    "        for i in range(1, 6):\n",
    "            with open(os.path.join(path, \"data_batch_%d.bin\" %i), mode='rb') as file:\n",
    "                t = np.fromfile(file, dtype = np.uint8).reshape(10000, -1)\n",
    "            file.close()\n",
    "            y = t[:, 0]\n",
    "            x = combineChannels(t)\n",
    "            if self.x_train is None: self.x_train = x\n",
    "            else: self.x_train = np.concatenate((self.x_train, x))\n",
    "                \n",
    "            if y_train is None: y_train = y\n",
    "            else: y_train = np.concatenate((y_train, y))\n",
    "        \n",
    "        self.start_batch_index = self.end_batch_index = self.num_train_images = np.shape(y_train)[0]\n",
    "        \n",
    "        with open(os.path.join(path, \"test_batch.bin\"), mode='rb') as file:\n",
    "            t = np.fromfile(file, dtype = np.uint8).reshape(10000, -1)\n",
    "            y_test = t[:, 0]\n",
    "            self.x_test = combineChannels(t)\n",
    "        \n",
    "        ## change labels to one-hot type:\n",
    "        if is_one_hot:\n",
    "            self.y_train = np.zeros((self.num_train_images, self.num_classes))\n",
    "            self.y_train[np.arange(self.num_train_images), y_train] = 1\n",
    "            self.y_test = np.zeros((10000, self.num_classes))\n",
    "            self.y_test[np.arange(10000), y_test] = 1\n",
    "        else:\n",
    "            self.y_train = y_train#.reshape(np.shape(y_train)[0], 1)\n",
    "            self.y_test = y_test#.reshape(np.shape(y_test)[0], 1)\n",
    "        \n",
    "    def get_next_batch(self):\n",
    "        if self.end_batch_index >= self.num_train_images:\n",
    "            #suffle\n",
    "            self.suffle()\n",
    "            self.start_batch_index = 0\n",
    "            self.end_batch_index = self.batch_size\n",
    "        \n",
    "        batch_x = self.x_train[self.start_batch_index : self.end_batch_index].astype(\n",
    "            np.float16 if config.use_float16 else np.float32)\n",
    "        batch_y = self.y_train[self.start_batch_index : self.end_batch_index]\n",
    "        batch_x /= 255.0\n",
    "        self.start_batch_index = self.end_batch_index\n",
    "        self.end_batch_index += self.batch_size\n",
    "        \n",
    "        return (batch_x, batch_y)\n",
    "    \n",
    "    \n",
    "    def suffle(self, is_training = True):\n",
    "        if is_training:\n",
    "            perm = np.random.permutation(self.num_train_images)\n",
    "            self.x_train = self.x_train[perm]\n",
    "            self.y_train = self.y_train[perm]\n",
    "        else:\n",
    "            perm = np.random.permutation(10000)\n",
    "            self.x_test = self.x_test[perm]\n",
    "            self.y_test = self.y_test[perm]\n",
    "            \n",
    "            \n",
    "dataset = Dataset()\n",
    "dataset.readData(config.data_path_dir, is_one_hot = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## BUILD GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# this is a simpler version of Tensorflow's 'official' version. See:\n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L102\n",
    "def batch_norm_wrapper(x, is_training, step):\n",
    "    \"\"\"\n",
    "    is_training: a boolean tensor\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('batch_norm') as scope:\n",
    "        gamma = tf.Variable(tf.ones([x.get_shape()[-1]]), trainable = True)\n",
    "        beta  = tf.Variable(tf.zeros([x.get_shape()[-1]]), trainable = True)\n",
    "        pop_mean = tf.Variable(tf.zeros([x.get_shape()[-1]]), trainable=False, name = \"pop_mean\")\n",
    "        pop_var  = tf.Variable(tf.constant(1.0, shape = [x.get_shape()[-1]]), trainable=False, name = \"pop_var\")\n",
    "\n",
    "    def using_batch_statistics():\n",
    "        batch_mean, batch_var = tf.nn.moments(x,[0])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * config.BN_decay + batch_mean * (1 - config.BN_decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * config.BN_decay + batch_var * (1 - config.BN_decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(x,\n",
    "                batch_mean, batch_var, beta, gamma, config.BN_epsilon)\n",
    "        \n",
    "    def using_global_statistics():\n",
    "        return tf.nn.batch_normalization(x,\n",
    "            pop_mean, pop_var, beta, gamma, config.BN_epsilon)\n",
    "    \n",
    "    return tf.cond(is_training, \n",
    "                   using_batch_statistics, \n",
    "                   using_global_statistics)\n",
    "\n",
    "\n",
    "def batch_norm_wrapper_cnn(x, n_out, is_training, step):\n",
    "    with tf.variable_scope('batch_norm'):\n",
    "        xs = x.get_shape()\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]), name = 'beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]), name = 'gamma', trainable=True)\n",
    "        pop_mean = tf.Variable(tf.zeros([n_out]), trainable=False, name = \"pop_mean\")\n",
    "        pop_var  = tf.Variable(tf.constant(1.0, shape = [n_out]), trainable=False, name = \"pop_var\")\n",
    "        \n",
    "    def using_batch_statistics():\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * config.BN_decay + batch_mean * (1 - config.BN_decay))\n",
    "        train_var  = tf.assign(pop_var,\n",
    "                               pop_var * config.BN_decay + batch_var * (1 - config.BN_decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(x,\n",
    "                batch_mean, batch_var, beta, gamma, config.BN_epsilon)\n",
    "        \n",
    "    def using_global_statistics():\n",
    "        return tf.nn.batch_normalization(x,\n",
    "            pop_mean, pop_var, beta, gamma, config.BN_epsilon)\n",
    "    \n",
    "    return tf.cond(is_training, \n",
    "                   using_batch_statistics, \n",
    "                   using_global_statistics)\n",
    "\n",
    "def variable_on_gpu(name, shape, initializer):\n",
    "    with tf.device('/gpu:0'):\n",
    "        dtype = tf.float16 if config.use_float16 else tf.float32\n",
    "        var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "    return var\n",
    "\n",
    "def variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    dtype = tf.float16 if config.use_float16 else tf.float32\n",
    "    var = variable_on_gpu(\n",
    "        name,\n",
    "        shape,\n",
    "        tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_graph(is_using_BN = True, is_using_dropout = False):\n",
    "    \"\"\"define place holder\"\"\"\n",
    "    dtype = tf.float16 if config.use_float16 else tf.float32\n",
    "    x = tf.placeholder(dtype, shape=[None, dataset.height, dataset.width, dataset.depth])\n",
    "    y = tf.placeholder(tf.int32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    weight_decay = 0.001\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    \"\"\"1st convolution layer\"\"\"\n",
    "    with tf.variable_scope('conv1_1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 3, 32], \n",
    "                                             stddev = np.sqrt(1 / (3 * 3 * 3)), \n",
    "                                             wd = weight_decay)\n",
    "        conv1_1 = tf.nn.conv2d(x, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv1_1 = batch_norm_wrapper_cnn(conv1_1, 32, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 32, tf.constant_initializer(0.01))\n",
    "            conv1_1 = tf.add(conv1_1, biases)\n",
    "        conv1_1 = tf.nn.relu(conv1_1)\n",
    "    \n",
    "    \n",
    "    \"\"\"2nd convolution layer\"\"\"\n",
    "    with tf.variable_scope('conv1_2') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 32, 32], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 32)), \n",
    "                                             wd = weight_decay)\n",
    "        conv1_2 = tf.nn.conv2d(conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv1_2 = batch_norm_wrapper_cnn(conv1_2, 32, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 32, tf.constant_initializer(0.01))\n",
    "            conv1_2 = tf.add(conv1_2, biases)\n",
    "        conv1_2 = tf.nn.relu(conv1_2)\n",
    "    \n",
    "    \n",
    "    \"\"\"3rd convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv1_3') as scope:\n",
    "        kernel = variable_with_weight_decay('weights',\n",
    "                                             shape = [3, 3, 32, 32],\n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 32)),\n",
    "                                             wd = weight_decay)\n",
    "        \n",
    "        conv1_3 = tf.nn.conv2d(conv1_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv1_3 = batch_norm_wrapper_cnn(conv1_3, 32, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 32, tf.constant_initializer(0.01))\n",
    "            conv1_3 = tf.add(conv1_3, biases)\n",
    "        conv1_3 = tf.nn.relu(conv1_3)\n",
    "        \n",
    "        pool1 = tf.nn.max_pool(conv1_3, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding = 'SAME', name = 'pool2')\n",
    "        \n",
    "    \n",
    "    \"\"\"4th convolution layer\"\"\"\n",
    "    with tf.variable_scope('conv2_1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 32, 64], \n",
    "                                             stddev = np.sqrt(1 / (3 * 3 * 32)), \n",
    "                                             wd = weight_decay)\n",
    "        conv2_1 = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv2_1 = batch_norm_wrapper_cnn(conv2_1, 64, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 64, tf.constant_initializer(0.01))\n",
    "            conv2_1 = tf.add(conv2_1, biases)\n",
    "        conv2_1 = tf.nn.relu(conv2_1)\n",
    "    \n",
    "    \n",
    "    \"\"\"5th convolution layer\"\"\"\n",
    "    with tf.variable_scope('conv2_2') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 64, 64], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 64)), \n",
    "                                             wd = weight_decay)\n",
    "        conv2_2 = tf.nn.conv2d(conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv2_2 = batch_norm_wrapper_cnn(conv2_2, 64, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 64, tf.constant_initializer(0.01))\n",
    "            conv2_2 = tf.add(conv2_2, biases)\n",
    "        conv2_2 = tf.nn.relu(conv2_2)\n",
    "    \n",
    "    \n",
    "    \"\"\"6th convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv2_3') as scope:\n",
    "        kernel = variable_with_weight_decay('weights',\n",
    "                                             shape = [3, 3, 64, 64],\n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 64)),\n",
    "                                             wd = weight_decay)\n",
    "        \n",
    "        conv2_3 = tf.nn.conv2d(conv2_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv2_3 = batch_norm_wrapper_cnn(conv2_3, 64, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 64, tf.constant_initializer(0.01))\n",
    "            conv2_3 = tf.add(conv2_3, biases)\n",
    "        conv2_3 = tf.nn.relu(conv2_3)\n",
    "        \n",
    "        pool2 = tf.nn.max_pool(conv2_3, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding = 'SAME', name = 'pool2')\n",
    "        \n",
    "    \n",
    "    \"\"\"7th convolution layer\"\"\"\n",
    "    with tf.variable_scope('conv3_1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 64, 128], \n",
    "                                             stddev = np.sqrt(1 / (3 * 3 * 64)), \n",
    "                                             wd = weight_decay)\n",
    "        conv3_1 = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv3_1 = batch_norm_wrapper_cnn(conv3_1, 128, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 128, tf.constant_initializer(0.01))\n",
    "            conv3_1 = tf.add(conv3_1, biases)\n",
    "        conv3_1 = tf.nn.relu(conv3_1)\n",
    "    \n",
    "    \n",
    "    \"\"\"8th convolution layer\"\"\"\n",
    "    with tf.variable_scope('conv3_2') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 128, 128], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 128)), \n",
    "                                             wd = weight_decay)\n",
    "        conv3_2 = tf.nn.conv2d(conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv3_2 = batch_norm_wrapper_cnn(conv3_2, 128, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 128, tf.constant_initializer(0.01))\n",
    "            conv3_2 = tf.add(conv3_2, biases)\n",
    "        conv3_2 = tf.nn.relu(conv3_2)\n",
    "    \n",
    "    \n",
    "    \"\"\"9th convolution layer with avg pooling\"\"\"\n",
    "    with tf.variable_scope('conv3_3') as scope:\n",
    "        conv3_3 = variable_with_weight_decay('weights',\n",
    "                                             shape = [3, 3, 128, 128],\n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 128)),\n",
    "                                             wd = weight_decay)\n",
    "        \n",
    "        conv3_3 = tf.nn.conv2d(conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv3_3 = batch_norm_wrapper_cnn(conv3_3, 128, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 128, tf.constant_initializer(0.01))\n",
    "            conv3_3 = tf.add(conv3_3, biases)\n",
    "        conv3_3 = tf.nn.relu(conv3_3)\n",
    "        \n",
    "        pool3 = tf.nn.avg_pool(conv3_3, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1],\n",
    "                         padding = 'SAME', name = 'pool9')\n",
    "            \n",
    "        pool3_flat = tf.reshape(pool3, [-1, 512])\n",
    "    \n",
    "    \n",
    "#     \"\"\"1st full connection layer\"\"\"\n",
    "    with tf.variable_scope('fc1') as scope:        \n",
    "        weights = variable_with_weight_decay('weights', \n",
    "                                             shape = [512, 512],\n",
    "                                             stddev = np.sqrt(1 / (512)), \n",
    "                                             wd = weight_decay)\n",
    "        fc1_logits = tf.matmul(pool3_flat, weights)\n",
    "        if is_using_BN:\n",
    "            fc1_logits = batch_norm_wrapper(fc1_logits, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', [512], tf.constant_initializer(0.01))\n",
    "            fc1_logits = tf.add(fc1_logits, biases, name = \"logits\")\n",
    "        fc1_out = tf.nn.relu(fc1_logits, name = \"after_activation\")\n",
    "        if is_using_dropout:\n",
    "            fc1_out = tf.contrib.layers.dropout(fc1_out, keep_prob = 0.6, is_training = is_training)\n",
    "    \n",
    "    \n",
    "    \"\"\"softmax layer\"\"\"\n",
    "    with tf.variable_scope('softmax') as scope:\n",
    "        weights = variable_with_weight_decay(name = 'weights', \n",
    "                                             shape = [512, dataset.num_classes],\n",
    "                                             stddev = np.sqrt(2 /(512)), \n",
    "                                             wd = weight_decay)\n",
    "        biases = variable_on_gpu('biases', [dataset.num_classes], tf.constant_initializer(0.01))\n",
    "        y_out = tf.add(tf.matmul(fc1_out, weights), biases, name = \"output\")\n",
    "\n",
    "    \n",
    "    \"\"\"Loss, Optimizer and Predictions\"\"\"\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=y_out, name='cross_entropy'))\n",
    "    tf.add_to_collection('losses', cross_entropy)\n",
    "    total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "    \n",
    "    learning_rate = tf.maximum(tf.train.exponential_decay(config.learning_rate, global_step, 1000, 0.95, staircase=True),\n",
    "                               0.01)\n",
    "    \n",
    "#     dw1 = tf.reduce_mean(tf.abs(tf.gradients(total_loss, kernel1)))\n",
    "#     dw5 = tf.reduce_mean(tf.abs(tf.gradients(total_loss, kernel5)))\n",
    "#     dw9 = tf.reduce_mean(tf.abs(tf.gradients(total_loss, kernel9)))\n",
    "    \n",
    "#     do1 = tf.reduce_mean(tf.abs(tf.gradients(total_loss, conv1)))\n",
    "#     do5 = tf.reduce_mean(tf.abs(tf.gradients(total_loss, conv5)))\n",
    "#     do9 = tf.reduce_mean(tf.abs(tf.gradients(total_loss, conv9)))\n",
    "\n",
    "\n",
    "    trainer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(total_loss, global_step = global_step)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.cast(tf.arg_max(y_out, 1), tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return (x, y, is_training, keep_prob), trainer, accuracy, total_loss, y_out, tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TRAIN NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(is_using_BN = True,\n",
    "          is_continue = False,\n",
    "          model_name = None,\n",
    "          current_step = 0,\n",
    "          max_steps = 50000,\n",
    "          eval_interval = 200,\n",
    "          save_interval = 5000):\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "            (x_train, y_train, is_training, keep_prob), trainer, accuracy, loss, y_out, saver \\\n",
    "            = build_graph(is_using_BN = is_using_BN, is_using_dropout = True)\n",
    "    \n",
    "    if is_continue:\n",
    "        \"\"\"load something\"\"\"\n",
    "        f = open(os.path.join(config.store_path, \"losses.bin\"), \"rb\")\n",
    "        losses = np.fromfile(f, dtype = np.float32)\n",
    "        f.close()\n",
    "        f = open(os.path.join(config.store_path, \"train_acc.bin\"), \"rb\")\n",
    "        train_acc = np.fromfile(f, dtype = np.float32)\n",
    "        f.close()\n",
    "        f = open(os.path.join(config.store_path, \"test_acc.bin\"), \"rb\")\n",
    "        test_acc = np.fromfile(f, dtype = np.float32)\n",
    "        f.close()\n",
    "    else:\n",
    "        losses, train_acc, test_acc = np.array([], dtype = np.float32), np.array([], dtype = np.float32), np.array([], dtype = np.float32)\n",
    "    \n",
    "    def evaluate(sess, num_examples = 10000, is_on_training_set = True): \n",
    "        accs, ls = [], []\n",
    "        if is_on_training_set:\n",
    "            prem = np.random.permutation(dataset.num_train_images)\n",
    "            x = dataset.x_train[prem[:num_examples]]\n",
    "            y = dataset.y_train[prem[:num_examples]]\n",
    "        else:\n",
    "            x = dataset.x_test[:num_examples]\n",
    "            y = dataset.y_test[:num_examples]\n",
    "        \n",
    "        for i in range(0, num_examples, 1000):\n",
    "            res = sess.run([accuracy, loss],\n",
    "                           feed_dict = {x_train: x[i : i + 1000] / 255.0,\n",
    "                                        y_train: y[i : i + 1000], \n",
    "                                        is_training: False,\n",
    "                                       keep_prob : 1})\n",
    "            accs.append(res[0])\n",
    "            ls.append(res[1])\n",
    "        \n",
    "        return sum(accs) / len(accs), sum(ls) / len(ls)\n",
    "\n",
    "    with tf.Session(graph = graph) as sess:\n",
    "        if is_continue:\n",
    "            if model_name is None:\n",
    "                raise ValueError('Need the name of model')\n",
    "            saver = tf.train.import_meta_graph(os.path.join(config.store_path, model_name + '.meta'))\n",
    "            saver.restore(sess, os.path.join(config.store_path, model_name))\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        def save(arr, name):\n",
    "            f = open(os.path.join(config.store_path, name+ \".bin\"), \"wb\")\n",
    "            arr.astype(np.float32).tofile(f)\n",
    "            f.close()\n",
    "        \n",
    "        for i in tqdm.tqdm(range(current_step + 1, max_steps + 1)):\n",
    "            batch = dataset.get_next_batch()\n",
    "            _,l = sess.run([trainer, loss], \\\n",
    "                            feed_dict = {x_train : batch[0],\n",
    "                                         y_train : batch[1],\n",
    "                                         is_training : True,\n",
    "                                         keep_prob: 0.6})\n",
    "\n",
    "            if i % save_interval == 0:\n",
    "                saved_model = saver.save(sess, config.store_path + 'model.ckpt', i)\n",
    "                save(losses, 'losses')\n",
    "                save(train_acc, 'train_acc')\n",
    "                save(test_acc, 'test_acc')\n",
    "                \n",
    "            \n",
    "            if i % eval_interval == 0:\n",
    "                test_acc = np.append(test_acc, evaluate(sess, 10000, False)[0]).astype(np.float32)\n",
    "                res = evaluate(sess)\n",
    "                train_acc = np.append(train_acc, res[0])\n",
    "                losses = np.append(losses, res[1])\n",
    "                \n",
    "                displayer.clear_output()\n",
    "                print(test_acc[-1], train_acc[-1])\n",
    "                plt.figure(1)\n",
    "                plt.plot(range(0, i, eval_interval), train_acc, 'red', range(0, i, eval_interval), test_acc, 'blue')\n",
    "                plt.figure(2)\n",
    "                plt.plot(range(0, i, eval_interval), losses)\n",
    "                plt.show()\n",
    "                \n",
    "        \n",
    "    return losses, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "config.learning_rate = 0.1\n",
    "config.store_path = \"../../../models/CNN/cifar10/withoutBN/lr 0.1\"\n",
    "losses, train_acc, test_acc = train(is_using_BN = False)\n",
    "''', is_continue = True, \n",
    "                                    model_name = 'model.ckpt-50000',\n",
    "                                    current_step = 50000)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0522857 0.0522857 0.0522857\n",
      "0.8574 1.00000011921\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x251824a6a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2520361c160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 50000/50000 [1:45:10<00:00,  1.05s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\", is_continue = True, \\n                                    model_name = 'model.ckpt-50000',\\n                                    current_step = 50000)\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.learning_rate = 0.1\n",
    "config.store_path = \"../../../models/CNN/cifar10/withBN/lr 0.1\"\n",
    "losses, train_acc, test_acc = train(is_using_BN = True)\n",
    "''', is_continue = True, \n",
    "                                    model_name = 'model.ckpt-50000',\n",
    "                                    current_step = 50000)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## RESTORE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "correct = []\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    (x_eval, y_eval, is_training), _, accuracy, y_out, loss, saver_eval = build_graph(is_using_BN=True)\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    for i in range(0, 2000, 500):\n",
    "        saver_eval = tf.train.import_meta_graph('../../models/CNN/cifar10/withBN/model.ckpt-50000.meta')\n",
    "        saver_eval.restore(sess, \"../../models/CNN/cifar10/withBN/model.ckpt-50000\")\n",
    "\n",
    "        cor = sess.run([accuracy],\n",
    "                       feed_dict={x_eval: dataset.x_train[i:i + 500] / 255.0, \n",
    "                                  y_eval: dataset.y_train[i:i + 500], \n",
    "                                  is_training: False})\n",
    "        correct.append(cor[0])\n",
    "        \n",
    "print (sum(correct) / len(correct))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
