{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## IMPORT, CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm, os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display as displayer\n",
    "import math\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import struct\n",
    "\n",
    "class Config(object):\n",
    "    pass\n",
    "config = Config()\n",
    "config.batch_size = 128\n",
    "config.learning_rate = 0.02\n",
    "config.use_float16 = False\n",
    "config.BN_epsilon = 1e-5\n",
    "config.BN_decay = 0.995\n",
    "\n",
    "#mnist = input_data.read_data_sets('../../datasets/MNIST', one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "testing\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "def read_MNIST(dataset = \"training\", path = \".\"):\n",
    "    \"\"\"\n",
    "    Python function for importing the MNIST data set.  It returns an iterator\n",
    "    of 2-tuples with the first element being the label and the second element\n",
    "    being a numpy.uint8 2D array of pixel data for the given image.\n",
    "    \"\"\"\n",
    "    print (dataset)\n",
    "    if dataset is \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images.idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels.idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images.idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels.idx1-ubyte')\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    # Load everything in some numpy arrays\n",
    "    with open(fname_lbl, 'rb') as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(fname_img, 'rb') as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows, cols, 1) / 255.0\n",
    "\n",
    "    return img, lbl\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self):\n",
    "        self.batch_size = config.batch_size\n",
    "        self.x_train, self.y_train = read_MNIST(dataset='training', path='../../../datasets/MNIST/')\n",
    "        self.x_test, self.y_test = read_MNIST(dataset='testing', path='../../../datasets/MNIST/')\n",
    "        self.start_batch_index = self.end_batch_index = self.num_train_images = np.shape(self.y_train)[0]\n",
    "        \n",
    "    def get_next_batch(self):\n",
    "        if self.start_batch_index == self.num_train_images:\n",
    "            #suffle\n",
    "            self.suffle()\n",
    "            self.start_batch_index = 0\n",
    "            self.end_batch_index = self.batch_size\n",
    "            \n",
    "        if self.end_batch_index > self.num_train_images:\n",
    "            #suffle\n",
    "            self.end_batch_index = self.num_train_images\n",
    "            self.start_batch_index = self.num_train_images - self.batch_size\n",
    "        \n",
    "        batch_x = self.x_train[self.start_batch_index : self.end_batch_index].astype(\n",
    "            np.float16 if config.use_float16 else np.float32)\n",
    "        batch_y = self.y_train[self.start_batch_index : self.end_batch_index]\n",
    "        self.start_batch_index = self.end_batch_index\n",
    "        self.end_batch_index += self.batch_size\n",
    "        \n",
    "        return (batch_x, batch_y)\n",
    "    \n",
    "    def suffle(self):\n",
    "        perm = np.random.permutation(self.num_train_images)\n",
    "        self.x_train = self.x_train[perm]\n",
    "        self.y_train = self.y_train[perm]\n",
    "        \n",
    "dataset = Dataset()\n",
    "print (np.shape(dataset.x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## BUILD GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batch_norm_wrapper(x, is_training, step):\n",
    "    \"\"\"\n",
    "    is_training: a boolean tensor\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('batch_norm') as scope:\n",
    "        gamma = tf.Variable(tf.ones([x.get_shape()[-1]]), trainable = True)\n",
    "        beta  = tf.Variable(tf.zeros([x.get_shape()[-1]]), trainable = True)\n",
    "        pop_mean = tf.Variable(tf.zeros([x.get_shape()[-1]]), trainable=False, name = \"pop_mean\")\n",
    "        pop_var  = tf.Variable(tf.constant(1.0, shape = [x.get_shape()[-1]]), trainable=False, name = \"pop_var\")\n",
    "\n",
    "    def using_batch_statistics():\n",
    "        batch_mean, batch_var = tf.nn.moments(x,[0])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * config.BN_decay + batch_mean * (1 - config.BN_decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * config.BN_decay + batch_var * (1 - config.BN_decay))\n",
    "#         mean = tf.clip_by_value(batch_mean, pop_mean / 3, pop_mean * 3)\n",
    "#         var = tf.clip_by_value(batch_var, pop_var - 2, pop_var + 2)\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "#             return tf.cond(tf.greater(step, 5000), lambda: tf.nn.batch_normalization(x, mean, var, beta, gamma, config.BN_epsilon),\n",
    "#                                                 lambda: tf.nn.batch_normalization(x, batch_mean, batch_var, beta, gamma, config.BN_epsilon))\n",
    "            return tf.nn.batch_normalization(x,\n",
    "                pop_mean, pop_var, beta, gamma, config.BN_epsilon)\n",
    "    def using_global_statistics():\n",
    "        return tf.nn.batch_normalization(x,\n",
    "            pop_mean, pop_var, beta, gamma, config.BN_epsilon)\n",
    "    \n",
    "    return tf.cond(is_training, \n",
    "                   using_batch_statistics, \n",
    "                   using_global_statistics)\n",
    "\n",
    "\n",
    "def batch_norm_wrapper_cnn(x, n_out, is_training, step):\n",
    "    with tf.variable_scope('batch_norm'):\n",
    "        xs = x.get_shape()\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
    "                                    name = 'beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
    "                                     name = 'gamma', trainable=True)\n",
    "        pop_mean = tf.Variable(tf.zeros([n_out]), trainable=False, name = \"pop_mean\")\n",
    "        pop_var  = tf.Variable(tf.constant(1.0, shape = [n_out]), trainable=False, name = \"pop_var\")\n",
    "        \n",
    "    def using_batch_statistics():\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * config.BN_decay + batch_mean * (1 - config.BN_decay))\n",
    "        train_var  = tf.assign(pop_var,\n",
    "                               pop_var * config.BN_decay + batch_var * (1 - config.BN_decay))\n",
    "#         mean = tf.clip_by_value(batch_mean, pop_mean / 3, pop_mean * 3)\n",
    "#         var = tf.clip_by_value(batch_var, pop_var - 2, pop_var + 2)\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(x, pop_mean, pop_var, beta, gamma, config.BN_epsilon)\n",
    "#             return tf.cond(tf.greater(step, 5000), lambda: tf.nn.batch_normalization(x, mean, var, beta, gamma, config.BN_epsilon),\n",
    "#                                                 lambda: tf.nn.batch_normalization(x, batch_mean, batch_var, beta, gamma, config.BN_epsilon))\n",
    "        \n",
    "    def using_global_statistics():\n",
    "        return tf.nn.batch_normalization(x,\n",
    "            pop_mean, pop_var, beta, gamma, config.BN_epsilon)\n",
    "    \n",
    "    return tf.cond(is_training, \n",
    "                   using_batch_statistics, \n",
    "                   using_global_statistics)\n",
    "\n",
    "def variable_on_cpu(name, shape, initializer):\n",
    "    with tf.device('/gpu:0'):\n",
    "        dtype = tf.float16 if config.use_float16 else tf.float32\n",
    "        var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "    return var\n",
    "\n",
    "def variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    dtype = tf.float16 if config.use_float16 else tf.float32\n",
    "    var = variable_on_cpu(\n",
    "        name,\n",
    "        shape,\n",
    "        tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_graph(is_using_BN = True, is_using_dropout = False):\n",
    "    \"\"\"define place holder\"\"\"\n",
    "    dtype = tf.float16 if config.use_float16 else tf.float32\n",
    "    x = tf.placeholder(dtype, shape=[None, 28, 28, 1])\n",
    "    y = tf.placeholder(tf.int32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    weight_decay = 0.001\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    #xx = tf.image.resize_images(x, [32, 32])\n",
    "    \n",
    "    \"\"\"1st convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv1_1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 1, 32], \n",
    "                                             stddev = np.sqrt(1 / (3 * 3 * 1)), \n",
    "                                             wd = weight_decay)\n",
    "        conv1_1 = tf.nn.conv2d(x, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv1_1 = batch_norm_wrapper_cnn(conv1_1, 32, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 32, tf.constant_initializer(0.01))\n",
    "            conv1_1 = tf.add(conv1_1, biases)\n",
    "        conv1_1 = tf.nn.relu(conv1_1)\n",
    "        \n",
    "    with tf.variable_scope('conv1_2') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 32, 32], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 32)), \n",
    "                                             wd = weight_decay)\n",
    "        conv1_2 = tf.nn.conv2d(conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv1_2 = batch_norm_wrapper_cnn(conv1_2, 32, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 32, tf.constant_initializer(0.01))\n",
    "            conv1_2 = tf.add(conv1_2, biases)\n",
    "        conv1_2 = tf.nn.relu(conv1_2)\n",
    "    \n",
    "    \n",
    "    \"\"\"2nd convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv1_3') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 32, 32], \n",
    "                                             stddev = np.sqrt(1 / (3 * 3 * 3)), \n",
    "                                             wd = weight_decay)\n",
    "        conv1_3 = tf.nn.conv2d(conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv1_3 = batch_norm_wrapper_cnn(conv1_3, 32, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 32, tf.constant_initializer(0.01))\n",
    "            conv1_3 = tf.add(conv1_3, biases)\n",
    "        conv1_3 = tf.nn.relu(conv1_3)\n",
    "    \n",
    "    \n",
    "    \"\"\"3rd convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv1_4') as scope:\n",
    "        kernel = variable_with_weight_decay('weights',\n",
    "                                             shape = [3, 3, 32, 32],\n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 32)),\n",
    "                                             wd = weight_decay)\n",
    "        \n",
    "        conv1_4 = tf.nn.conv2d(conv1_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv1_4 = batch_norm_wrapper_cnn(conv1_4, 32, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 32, tf.constant_initializer(0.01))\n",
    "            conv1_4 = tf.add(conv1_4, biases)\n",
    "        conv1_4 = tf.nn.relu(conv1_4)\n",
    "        pool1 = tf.nn.max_pool(conv1_4, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding = 'SAME', name = 'pool2')\n",
    "#         if is_using_BN:\n",
    "#             pool1 = batch_norm_wrapper_cnn(pool1, 32, is_training)\n",
    "        \n",
    "    \n",
    "    \"\"\"1st convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv2_1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 32, 64], \n",
    "                                             stddev = np.sqrt(1 / (3 * 3 * 64)), \n",
    "                                             wd = weight_decay)\n",
    "        conv2_1 = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv2_1 = batch_norm_wrapper_cnn(conv2_1, 64, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 64, tf.constant_initializer(0.01))\n",
    "            conv2_1 = tf.add(conv2_1, biases)\n",
    "        conv2_1 = tf.nn.relu(conv2_1)\n",
    "    \n",
    "    \n",
    "    \"\"\"2nd convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv2_2') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 64, 64], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 64)), \n",
    "                                             wd = weight_decay)\n",
    "        conv2_2 = tf.nn.conv2d(conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv2_2 = batch_norm_wrapper_cnn(conv2_2, 64, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 64, tf.constant_initializer(0.01))\n",
    "            conv2_2 = tf.add(conv2_2, biases)\n",
    "        conv2_2 = tf.nn.relu(conv2_2)\n",
    "        \n",
    "    with tf.variable_scope('conv2_3') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 64, 64], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 64)), \n",
    "                                             wd = weight_decay)\n",
    "        conv2_3 = tf.nn.conv2d(conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv2_3 = batch_norm_wrapper_cnn(conv2_3, 64, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 64, tf.constant_initializer(0.01))\n",
    "            conv2_3 = tf.add(conv2_3, biases)\n",
    "        conv2_3 = tf.nn.relu(conv2_3)\n",
    "    \n",
    "    \n",
    "    \"\"\"3rd convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv2_4') as scope:\n",
    "        kernel = variable_with_weight_decay('weights',\n",
    "                                             shape = [3, 3, 64, 64],\n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 64)),\n",
    "                                             wd = weight_decay)\n",
    "        \n",
    "        conv2_4 = tf.nn.conv2d(conv2_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv2_4 = batch_norm_wrapper_cnn(conv2_4, 64, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 64, tf.constant_initializer(0.01))\n",
    "            conv2_4 = tf.add(conv2_4, biases)\n",
    "        conv2_4 = tf.nn.relu(conv2_4)\n",
    "        \n",
    "        pool2 = tf.nn.max_pool(conv2_4, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding = 'SAME', name = 'pool2')\n",
    "#         if is_using_BN:\n",
    "#             pool2 = batch_norm_wrapper_cnn(pool2, 64, is_training)\n",
    "        \n",
    "    \n",
    "    with tf.variable_scope('conv3_1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 64, 128], \n",
    "                                             stddev = np.sqrt(1 / (3 * 3 * 64)), \n",
    "                                             wd = weight_decay)\n",
    "        conv3_1 = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv3_1 = batch_norm_wrapper_cnn(conv3_1, 128, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 128, tf.constant_initializer(0.01))\n",
    "            conv3_1 = tf.add(conv3_1, biases)\n",
    "        conv3_1 = tf.nn.relu(conv3_1)\n",
    "    \n",
    "    \n",
    "    \"\"\"2nd convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv3_2') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 128, 128], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 128)), \n",
    "                                             wd = weight_decay)\n",
    "        conv3_2 = tf.nn.conv2d(conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv3_2 = batch_norm_wrapper_cnn(conv3_2, 128, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 128, tf.constant_initializer(0.01))\n",
    "            conv3_2 = tf.add(conv3_2, biases)\n",
    "        conv3_2 = tf.nn.relu(conv3_2)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('conv3_3') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 128, 128], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 128)), \n",
    "                                             wd = weight_decay)\n",
    "        conv3_3 = tf.nn.conv2d(conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv3_3 = batch_norm_wrapper_cnn(conv3_3, 128, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 128, tf.constant_initializer(0.01))\n",
    "            conv3_3 = tf.add(conv3_3, biases)\n",
    "        conv3_3 = tf.nn.relu(conv3_3)\n",
    "    \n",
    "    \n",
    "    \"\"\"3rd convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv3_4') as scope:\n",
    "        kernel = variable_with_weight_decay('weights',\n",
    "                                             shape = [3, 3, 128, 128],\n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 128)),\n",
    "                                             wd = weight_decay)\n",
    "        \n",
    "        conv3_4 = tf.nn.conv2d(conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv3_4 = batch_norm_wrapper_cnn(conv3_4, 128, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 128, tf.constant_initializer(0.01))\n",
    "            conv3_4 = tf.add(conv3_4, biases)\n",
    "        conv3_4 = tf.nn.relu(conv3_4)\n",
    "        \n",
    "        pool3 = tf.nn.avg_pool(conv3_4, ksize=[1, 4, 4, 1], strides=[1, 3, 3, 1],\n",
    "                         padding = 'VALID', name = 'pool9')\n",
    "#         if is_using_BN:\n",
    "#             pool3 = batch_norm_wrapper_cnn(pool3, 128, is_training)\n",
    "            \n",
    "        pool3_flat = tf.reshape(pool3, [-1, 512])\n",
    "    \n",
    "    \n",
    "#     \"\"\"1st full connection layer\"\"\"\n",
    "    with tf.variable_scope('fc1') as scope:\n",
    "#         pool4_flat = tf.reshape(pool4, [-1, 2048])\n",
    "#         if is_using_dropout:\n",
    "#             pool4_flat = tf.contrib.layers.dropout(pool4_flat, keep_prob = 0.8, is_training = is_training)\n",
    "        \n",
    "        weights = variable_with_weight_decay('weights', \n",
    "                                             shape = [512, 512],\n",
    "                                             stddev = np.sqrt(2 / (512)), \n",
    "                                             wd = 0.001)\n",
    "        if is_using_BN:\n",
    "            fc1_logits = batch_norm_wrapper(tf.matmul(pool3_flat, weights), is_training = is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', [512], tf.constant_initializer(0.01))\n",
    "            fc1_logits = tf.add(tf.matmul(pool3_flat, weights), \n",
    "                                         biases, name = \"logits\")\n",
    "        fc1_out = tf.nn.relu(fc1_logits, name = \"after_activation\")\n",
    "        if is_using_dropout:\n",
    "            fc1_out = tf.contrib.layers.dropout(fc1_out, keep_prob = 0.6, is_training = is_training)\n",
    "    \n",
    "\n",
    "    \n",
    "    \"\"\"last full connection layer\"\"\"\n",
    "    with tf.variable_scope('softmax1') as scope:\n",
    "        weights = variable_with_weight_decay(name = 'weights', \n",
    "                                             shape = [512, 10],\n",
    "                                             stddev = np.sqrt(2 /(512)), \n",
    "                                             wd = weight_decay)\n",
    "        biases = variable_on_cpu('biases', [10],\n",
    "                              tf.constant_initializer(0.01))\n",
    "        y_out = tf.add(tf.matmul(fc1_out, weights), biases, name = \"output\")\n",
    "        \n",
    "    \n",
    "    \"\"\"Loss, Optimizer and Predictions\"\"\"\n",
    "    #y = tf.cast(y, tf.int64)\n",
    "    cross_entropy1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=y_out, name='cross_entropy'))\n",
    "    tf.add_to_collection('losses', cross_entropy1)\n",
    "    \n",
    "    total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "    \n",
    "    learning_rate = tf.maximum(tf.train.exponential_decay(config.learning_rate, global_step,\n",
    "                                           1000, 0.95, staircase=True), 0.01)\n",
    "\n",
    "    trainer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(total_loss, global_step = global_step)\n",
    "    #trainer = tf.train.MomentumOptimizer(learning_rate, momentum).minimize(total_loss, global_step = global_step)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.cast(tf.arg_max(y_out, 1), tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return (x, y, is_training), trainer, accuracy, total_loss, y_out, tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(is_using_BN = True,\n",
    "          is_continue = False,\n",
    "          model_name = None,\n",
    "          current_step = 0,\n",
    "          max_steps = 60000,\n",
    "          eval_interval = 200,\n",
    "          save_interval = 10000):\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        if is_using_BN:\n",
    "            (x_train, y_train, is_training), trainer, accuracy, loss, y_out, saver = build_graph(is_using_BN = True,\n",
    "                                                                                    is_using_dropout = True)\n",
    "        else:\n",
    "            (x_train, y_train, is_training), trainer, accuracy, loss, y_out, saver = build_graph(is_using_BN = False,\n",
    "                                                                                    is_using_dropout = True)\n",
    "    \n",
    "    if is_continue:\n",
    "        \"\"\"load something\"\"\"\n",
    "        f = open(os.path.join(config.store_path, \"losses.bin\"), \"rb\")\n",
    "        losses = np.fromfile(f, dtype = np.float32)\n",
    "        f.close()\n",
    "        f = open(os.path.join(config.store_path, \"train_acc.bin\"), \"rb\")\n",
    "        train_acc = np.fromfile(f, dtype = np.float32)\n",
    "        f.close()\n",
    "        f = open(os.path.join(config.store_path, \"test_acc.bin\"), \"rb\")\n",
    "        test_acc = np.fromfile(f, dtype = np.float32)\n",
    "        f.close()\n",
    "    else:\n",
    "        losses, train_acc, test_acc = np.array([], dtype = np.float32), np.array([], dtype = np.float32), np.array([], dtype = np.float32)\n",
    "    \n",
    "    def evaluate(sess, num_examples = 10000, is_on_training_set = True): \n",
    "        accs = []\n",
    "        if is_on_training_set:\n",
    "            prem = np.random.permutation(60000)\n",
    "            x = dataset.x_train[prem[:num_examples]]\n",
    "            y = dataset.y_train[prem[:num_examples]]\n",
    "        else:\n",
    "            x = dataset.x_test[:num_examples]\n",
    "            y = dataset.y_test[:num_examples]\n",
    "        \n",
    "        for i in range(0, num_examples, 500):\n",
    "            res = sess.run([accuracy],\n",
    "                           feed_dict = {x_train: x[i : i + 500],\n",
    "                                        y_train: y[i : i + 500], \n",
    "                                        is_training: False})\n",
    "            accs.append(res[0])\n",
    "        \n",
    "        return sum(accs) / len(accs)\n",
    "\n",
    "#     popvar_FC1 = []\n",
    "#     popmean_FC1 = []\n",
    "    with tf.Session(graph = graph) as sess:\n",
    "        if is_continue:\n",
    "            if model_name is None:\n",
    "                raise ValueError('Need the name of model')\n",
    "            saver = tf.train.import_meta_graph(os.path.join(config.store_path, model_name + '.meta'))\n",
    "            saver.restore(sess, os.path.join(config.store_path, model_name))\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        stop = 0    \n",
    "        for i in tqdm.tqdm(range(current_step + 1, max_steps + 1)):\n",
    "            batch = dataset.get_next_batch()\n",
    "            _,l, acc = sess.run([trainer, loss, accuracy], feed_dict = {x_train : batch[0],\n",
    "                                             y_train : batch[1],\n",
    "                                             is_training : True})\n",
    "            #losses.append(l)\n",
    "#             popvar_FC1.append(pvfc1)\n",
    "#             popmean_FC1.append(pmfc1)\n",
    "            if i % save_interval == 0:\n",
    "                saved_model = saver.save(sess, config.store_path + 'model.ckpt', i)\n",
    "                f = open(os.path.join(config.store_path, \"losses.bin\"), \"wb\")\n",
    "                losses.tofile(f)\n",
    "                f.close()\n",
    "                f = open(os.path.join(config.store_path, \"train_acc.bin\"), \"wb\")\n",
    "                train_acc.tofile(f)\n",
    "                f.close()\n",
    "                f = open(os.path.join(config.store_path, \"test_acc.bin\"), \"wb\")\n",
    "                test_acc.tofile(f)\n",
    "                f.close()\n",
    "                \n",
    "            \n",
    "            if i % eval_interval == 0:\n",
    "                losses = np.append(losses, l)\n",
    "                test_acc = np.append(test_acc, evaluate(sess, 10000, False)).astype(np.float32)\n",
    "                train_acc = np.append(train_acc, evaluate(sess)).astype(np.float32)\n",
    "                \n",
    "                \n",
    "                displayer.clear_output()\n",
    "                print(test_acc[-1])\n",
    "                print(train_acc[-1])\n",
    "                print(l)\n",
    "                plt.figure(1)\n",
    "                plt.plot(range(0, i, eval_interval), train_acc, 'red', range(0, i, eval_interval), test_acc, 'blue')\n",
    "                plt.figure(2)\n",
    "                plt.plot(range(0, i, eval_interval), losses)\n",
    "#                 ax1 = plt.subplot(111)\n",
    "#                 #plt.plot(np.arange(0, i, eval_interval), losses)\n",
    "#                 #plt.figure(1)\n",
    "#                 ax1.plot(range(0, i, eval_interval), train_acc, 'red', range(0, i, eval_interval), test_acc, 'blue')\n",
    "#                 plt.figure(2)\n",
    "#                 plt.plot(np.arange(0, i), popvar_FC1)\n",
    "#                 plt.figure(3)\n",
    "#                 plt.plot(np.arange(0, i), popmean_FC1)\n",
    "                plt.show()\n",
    "                \n",
    "        \n",
    "    return losses, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7854\n",
      "0.7705\n",
      "1.91807\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDARJEmAgSJIaA0GSBMD/B+1VAFuSlj5cAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20dcf8d2f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSWoMBEkSAP8PYIw4DEGxMfUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20dd0664ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                       | 261/60000 [00:23<1:12:01, 13.82it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-33141be251aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../../../models/CNN/MNIST/withoutBN/lr 0.1\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_using_BN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-256b278d24d1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(is_using_BN, is_continue, model_name, current_step, max_steps, eval_interval, save_interval)\u001b[0m\n\u001b[0;32m     64\u001b[0m             _,l, acc = sess.run([trainer, loss, accuracy], feed_dict = {x_train : batch[0],\n\u001b[0;32m     65\u001b[0m                                              \u001b[0my_train\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                                              is_training : True})\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;31m#losses.append(l)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;31m#             popvar_FC1.append(pvfc1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SupermanI81\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SupermanI81\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SupermanI81\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\SupermanI81\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SupermanI81\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 13.7 it/s\n",
    "config.learning_rate = 0.1\n",
    "config.store_path = \"../../../models/CNN/MNIST/withoutBN/lr 0.1\"\n",
    "losses, train_acc, test_acc = train(is_using_BN = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9965\n",
      "1.0\n",
      "0.0172727\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNORwNzObhv4/HjPh007FUXYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x211a30280f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2114acdf400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 60000/60000 [2:10:21<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# 8.6\n",
    "config.learning_rate = 0.1\n",
    "config.store_path = \"../../../models/CNN/MNIST/withBN/lr 0.1\"\n",
    "losses, train_acc, test_acc = train(is_using_BN = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TRAIN NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## RESTORE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00000011921\n"
     ]
    }
   ],
   "source": [
    "#sess = tf.InteractiveSession()\n",
    "#sess.close()\n",
    "\n",
    "correct = []\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    (x_eval, y_eval, is_training), _, accuracy, y_out, loss, saver_eval = build_graph(is_using_BN=True)\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    for i in range(0, 2000, 500):\n",
    "        saver_eval = tf.train.import_meta_graph('../../models/CNN/mnist/withBN/model.ckpt-50000.meta')\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        saver_eval.restore(sess, \"../../models/CNN/mnist/withBN/model.ckpt-50000\")\n",
    "\n",
    "        cor = sess.run([accuracy],\n",
    "                       feed_dict={x_eval: dataset.x_train[i:i + 500], \n",
    "                                  y_eval: dataset.y_train[i:i + 500], \n",
    "                                  is_training: False})\n",
    "        correct.append(cor[0])\n",
    "        \n",
    "print (sum(correct) / len(correct))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
