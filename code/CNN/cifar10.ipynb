{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## IMPORT, CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm, os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display as displayer\n",
    "import math\n",
    "\n",
    "class Config(object):\n",
    "    pass\n",
    "config = Config()\n",
    "config.batch_size = 128\n",
    "config.learning_rate = 0.1\n",
    "config.use_float16 = False\n",
    "# change data directory here\n",
    "config.data_path_dir = \"../../datasets/cifar-10-batches-bin\"\n",
    "config.BN_epsilon = 1e-5\n",
    "config.BN_decay = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, batch_size = config.batch_size):\n",
    "        self.width = 32\n",
    "        self.height = 32\n",
    "        self.depth = 3\n",
    "        self.x_train, self.y_train = None, None\n",
    "        self.x_test, self.y_test = None, None\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = 10\n",
    "        \n",
    "    def readData(self, path, is_one_hot = True):\n",
    "        def combineChannels(t):\n",
    "            r = t[:, 1:1025].reshape((10000, 1024, 1))\n",
    "            g = t[:, 1025:2049].reshape((10000, 1024, 1))\n",
    "            b = t[:, 2049:].reshape((10000, 1024, 1))\n",
    "            return np.dstack((r, g, b)).reshape((10000, 32, 32, 3))\n",
    "            \n",
    "        \n",
    "        y_train, y_test = None, None\n",
    "        for i in range(1, 6):\n",
    "            with open(os.path.join(path, \"data_batch_%d.bin\" %i), mode='rb') as file:\n",
    "                t = np.fromfile(file, dtype = np.uint8).reshape(10000, -1) / 255\n",
    "            file.close()\n",
    "            y = t[:, 0]\n",
    "            x = combineChannels(t)\n",
    "            if self.x_train is None: self.x_train = x\n",
    "            else: self.x_train = np.concatenate((self.x_train, x))\n",
    "                \n",
    "            if y_train is None: y_train = y\n",
    "            else: y_train = np.concatenate((y_train, y))\n",
    "        \n",
    "        self.start_batch_index = self.end_batch_index = self.num_train_images = np.shape(y_train)[0]\n",
    "        \n",
    "        with open(os.path.join(path, \"test_batch.bin\"), mode='rb') as file:\n",
    "            t = np.fromfile(file, dtype = np.uint8).reshape(10000, -1) / 255\n",
    "            y_test = t[:, 0]\n",
    "            self.x_test = combineChannels(t)\n",
    "        \n",
    "        ## change labels to one-hot type:\n",
    "        if is_one_hot:\n",
    "            self.y_train = np.zeros((self.num_train_images, self.num_classes))\n",
    "            self.y_train[np.arange(self.num_train_images), y_train] = 1\n",
    "            self.y_test = np.zeros((10000, self.num_classes))\n",
    "            self.y_test[np.arange(10000), y_test] = 1\n",
    "        else:\n",
    "            self.y_train = y_train#.reshape(np.shape(y_train)[0], 1)\n",
    "            self.y_test = y_test#.reshape(np.shape(y_test)[0], 1)\n",
    "        \n",
    "    def get_next_batch(self):\n",
    "        if self.end_batch_index >= self.num_train_images:\n",
    "            #suffle\n",
    "            self.suffle()\n",
    "            self.start_batch_index = 0\n",
    "            self.end_batch_index = self.batch_size\n",
    "        \n",
    "        batch_x = self.x_train[self.start_batch_index : self.end_batch_index].astype(\n",
    "            np.float16 if config.use_float16 else np.float32)\n",
    "        batch_y = self.y_train[self.start_batch_index : self.end_batch_index]\n",
    "        self.start_batch_index = self.end_batch_index\n",
    "        self.end_batch_index += self.batch_size\n",
    "        \n",
    "        return (batch_x, batch_y)\n",
    "    \n",
    "    \n",
    "    def suffle(self, is_training = True):\n",
    "        if is_training:\n",
    "            perm = np.random.permutation(self.num_train_images)\n",
    "            self.x_train = self.x_train[perm]\n",
    "            self.y_train = self.y_train[perm]\n",
    "        else:\n",
    "            perm = np.random.permutation(10000)\n",
    "            self.x_test = self.x_test[perm]\n",
    "            self.y_test = self.y_test[perm]\n",
    "            \n",
    "            \n",
    "dataset = Dataset()\n",
    "dataset.readData(config.data_path_dir, is_one_hot = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## BUILD GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# this is a simpler version of Tensorflow's 'official' version. See:\n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L102\n",
    "def batch_norm_wrapper(x, is_training, step):\n",
    "    \"\"\"\n",
    "    is_training: a boolean tensor\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('batch_norm') as scope:\n",
    "        gamma = tf.Variable(tf.ones([x.get_shape()[-1]]), trainable = True)\n",
    "        beta  = tf.Variable(tf.zeros([x.get_shape()[-1]]), trainable = True)\n",
    "        pop_mean = tf.Variable(tf.zeros([x.get_shape()[-1]]), trainable=False, name = \"pop_mean\")\n",
    "        pop_var  = tf.Variable(tf.constant(1.0, shape = [x.get_shape()[-1]]), trainable=False, name = \"pop_var\")\n",
    "\n",
    "    def using_batch_statistics():\n",
    "        batch_mean, batch_var = tf.nn.moments(x,[0])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * config.BN_decay + batch_mean * (1 - config.BN_decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * config.BN_decay + batch_var * (1 - config.BN_decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(x,\n",
    "                batch_mean, batch_var, beta, gamma, config.BN_epsilon)\n",
    "        \n",
    "    def using_global_statistics():\n",
    "        return tf.nn.batch_normalization(x,\n",
    "            pop_mean, pop_var, beta, gamma, config.BN_epsilon)\n",
    "    \n",
    "    return tf.cond(is_training, \n",
    "                   using_batch_statistics, \n",
    "                   using_global_statistics)\n",
    "\n",
    "\n",
    "def batch_norm_wrapper_cnn(x, n_out, is_training, step):\n",
    "    with tf.variable_scope('batch_norm'):\n",
    "        xs = x.get_shape()\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]), name = 'beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]), name = 'gamma', trainable=True)\n",
    "        pop_mean = tf.Variable(tf.zeros([n_out]), trainable=False, name = \"pop_mean\")\n",
    "        pop_var  = tf.Variable(tf.constant(1.0, shape = [n_out]), trainable=False, name = \"pop_var\")\n",
    "        \n",
    "    def using_batch_statistics():\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * config.BN_decay + batch_mean * (1 - config.BN_decay))\n",
    "        train_var  = tf.assign(pop_var,\n",
    "                               pop_var * config.BN_decay + batch_var * (1 - config.BN_decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(x,\n",
    "                batch_mean, batch_var, beta, gamma, config.BN_epsilon)\n",
    "        \n",
    "    def using_global_statistics():\n",
    "        return tf.nn.batch_normalization(x,\n",
    "            pop_mean, pop_var, beta, gamma, config.BN_epsilon)\n",
    "    \n",
    "    return tf.cond(is_training, \n",
    "                   using_batch_statistics, \n",
    "                   using_global_statistics)\n",
    "\n",
    "def variable_on_gpu(name, shape, initializer):\n",
    "    with tf.device('/gpu:0'):\n",
    "        dtype = tf.float16 if config.use_float16 else tf.float32\n",
    "        var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "    return var\n",
    "\n",
    "def variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    dtype = tf.float16 if config.use_float16 else tf.float32\n",
    "    var = variable_on_gpu(\n",
    "        name,\n",
    "        shape,\n",
    "        tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_graph(is_using_BN = True, is_using_dropout = False):\n",
    "    \"\"\"define place holder\"\"\"\n",
    "    dtype = tf.float16 if config.use_float16 else tf.float32\n",
    "    x = tf.placeholder(dtype, shape=[None, dataset.height, dataset.width, dataset.depth])\n",
    "    y = tf.placeholder(tf.int32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    weight_decay = 0.001\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    \"\"\"1st convolution layer\"\"\"\n",
    "    with tf.variable_scope('conv1_1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 3, 32], \n",
    "                                             stddev = np.sqrt(1 / (3 * 3 * 3)), \n",
    "                                             wd = weight_decay)\n",
    "        conv1_1 = tf.nn.conv2d(x, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv1_1 = batch_norm_wrapper_cnn(conv1_1, 32, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 32, tf.constant_initializer(0.01))\n",
    "            conv1_1 = tf.add(conv1_1, biases)\n",
    "        conv1_1 = tf.nn.relu(conv1_1)\n",
    "    \n",
    "    \n",
    "    \"\"\"2nd convolution layer\"\"\"\n",
    "    with tf.variable_scope('conv1_2') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 32, 32], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 32)), \n",
    "                                             wd = weight_decay)\n",
    "        conv1_2 = tf.nn.conv2d(conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv1_2 = batch_norm_wrapper_cnn(conv1_2, 32, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 32, tf.constant_initializer(0.01))\n",
    "            conv1_2 = tf.add(conv1_2, biases)\n",
    "        conv1_2 = tf.nn.relu(conv1_2)\n",
    "    \n",
    "    \n",
    "    \"\"\"3rd convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv1_3') as scope:\n",
    "        kernel = variable_with_weight_decay('weights',\n",
    "                                             shape = [3, 3, 32, 32],\n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 32)),\n",
    "                                             wd = weight_decay)\n",
    "        \n",
    "        conv1_3 = tf.nn.conv2d(conv1_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv1_3 = batch_norm_wrapper_cnn(conv1_3, 32, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 32, tf.constant_initializer(0.01))\n",
    "            conv1_3 = tf.add(conv1_3, biases)\n",
    "        conv1_3 = tf.nn.relu(conv1_3)\n",
    "        \n",
    "        pool1 = tf.nn.max_pool(conv1_3, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding = 'SAME', name = 'pool2')\n",
    "        \n",
    "    \n",
    "    \"\"\"4th convolution layer\"\"\"\n",
    "    with tf.variable_scope('conv2_1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 32, 64], \n",
    "                                             stddev = np.sqrt(1 / (3 * 3 * 32)), \n",
    "                                             wd = weight_decay)\n",
    "        conv2_1 = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv2_1 = batch_norm_wrapper_cnn(conv2_1, 64, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 64, tf.constant_initializer(0.01))\n",
    "            conv2_1 = tf.add(conv2_1, biases)\n",
    "        conv2_1 = tf.nn.relu(conv2_1)\n",
    "    \n",
    "    \n",
    "    \"\"\"5th convolution layer\"\"\"\n",
    "    with tf.variable_scope('conv2_2') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 64, 64], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 64)), \n",
    "                                             wd = weight_decay)\n",
    "        conv2_2 = tf.nn.conv2d(conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv2_2 = batch_norm_wrapper_cnn(conv2_2, 64, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 64, tf.constant_initializer(0.01))\n",
    "            conv2_2 = tf.add(conv2_2, biases)\n",
    "        conv2_2 = tf.nn.relu(conv2_2)\n",
    "    \n",
    "    \n",
    "    \"\"\"6th convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv2_3') as scope:\n",
    "        kernel = variable_with_weight_decay('weights',\n",
    "                                             shape = [3, 3, 64, 64],\n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 64)),\n",
    "                                             wd = weight_decay)\n",
    "        \n",
    "        conv2_3 = tf.nn.conv2d(conv2_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv2_3 = batch_norm_wrapper_cnn(conv2_3, 64, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 64, tf.constant_initializer(0.01))\n",
    "            conv2_3 = tf.add(conv2_3, biases)\n",
    "        conv2_3 = tf.nn.relu(conv2_3)\n",
    "        \n",
    "        pool2 = tf.nn.max_pool(conv2_3, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding = 'SAME', name = 'pool2')\n",
    "        \n",
    "    \n",
    "    \"\"\"7th convolution layer\"\"\"\n",
    "    with tf.variable_scope('conv3_1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 64, 128], \n",
    "                                             stddev = np.sqrt(1 / (3 * 3 * 64)), \n",
    "                                             wd = weight_decay)\n",
    "        conv3_1 = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv3_1 = batch_norm_wrapper_cnn(conv3_1, 128, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 128, tf.constant_initializer(0.01))\n",
    "            conv3_1 = tf.add(conv3_1, biases)\n",
    "        conv3_1 = tf.nn.relu(conv3_1)\n",
    "    \n",
    "    \n",
    "    \"\"\"8th convolution layer\"\"\"\n",
    "    with tf.variable_scope('conv3_2') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 128, 128], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 128)), \n",
    "                                             wd = weight_decay)\n",
    "        conv3_2 = tf.nn.conv2d(conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv3_2 = batch_norm_wrapper_cnn(conv3_2, 128, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 128, tf.constant_initializer(0.01))\n",
    "            conv3_2 = tf.add(conv3_2, biases)\n",
    "        conv3_2 = tf.nn.relu(conv3_2)\n",
    "    \n",
    "    \n",
    "    \"\"\"9th convolution layer with avg pooling\"\"\"\n",
    "    with tf.variable_scope('conv3_3') as scope:\n",
    "        conv3_3 = variable_with_weight_decay('weights',\n",
    "                                             shape = [3, 3, 128, 128],\n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 128)),\n",
    "                                             wd = weight_decay)\n",
    "        \n",
    "        conv3_3 = tf.nn.conv2d(conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv3_3 = batch_norm_wrapper_cnn(conv3_3, 128, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', 128, tf.constant_initializer(0.01))\n",
    "            conv3_3 = tf.add(conv3_3, biases)\n",
    "        conv3_3 = tf.nn.relu(conv3_3)\n",
    "        \n",
    "        pool3 = tf.nn.avg_pool(conv3_3, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1],\n",
    "                         padding = 'SAME', name = 'pool9')\n",
    "            \n",
    "        pool3_flat = tf.reshape(pool3, [-1, 512])\n",
    "    \n",
    "    \n",
    "#     \"\"\"1st full connection layer\"\"\"\n",
    "    with tf.variable_scope('fc1') as scope:        \n",
    "        weights = variable_with_weight_decay('weights', \n",
    "                                             shape = [512, 512],\n",
    "                                             stddev = np.sqrt(1 / (512)), \n",
    "                                             wd = weight_decay)\n",
    "        fc1_logits = tf.matmul(pool3_flat, weights)\n",
    "        if is_using_BN:\n",
    "            fc1_logits = batch_norm_wrapper(fc1_logits, is_training, global_step)\n",
    "        else:\n",
    "            biases = variable_on_gpu('biases', [512], tf.constant_initializer(0.01))\n",
    "            fc1_logits = tf.add(fc1_logits, biases, name = \"logits\")\n",
    "        fc1_out = tf.nn.relu(fc1_logits, name = \"after_activation\")\n",
    "        if is_using_dropout:\n",
    "            fc1_out = tf.contrib.layers.dropout(fc1_out, keep_prob = 0.6, is_training = is_training)\n",
    "    \n",
    "    \n",
    "    \"\"\"softmax layer\"\"\"\n",
    "    with tf.variable_scope('softmax') as scope:\n",
    "        weights = variable_with_weight_decay(name = 'weights', \n",
    "                                             shape = [512, dataset.num_classes],\n",
    "                                             stddev = np.sqrt(2 /(512)), \n",
    "                                             wd = weight_decay)\n",
    "        biases = variable_on_gpu('biases', [dataset.num_classes], tf.constant_initializer(0.01))\n",
    "        y_out = tf.add(tf.matmul(fc1_out, weights), biases, name = \"output\")\n",
    "\n",
    "    \n",
    "    \"\"\"Loss, Optimizer and Predictions\"\"\"\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=y_out, name='cross_entropy'))\n",
    "    tf.add_to_collection('losses', cross_entropy)\n",
    "    total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "    \n",
    "    learning_rate = tf.maximum(tf.train.exponential_decay(config.learning_rate, global_step, 1000, 0.95, staircase=True),\n",
    "                               0.01)\n",
    "    \n",
    "#     dw1 = tf.reduce_mean(tf.abs(tf.gradients(total_loss, kernel1)))\n",
    "#     dw5 = tf.reduce_mean(tf.abs(tf.gradients(total_loss, kernel5)))\n",
    "#     dw9 = tf.reduce_mean(tf.abs(tf.gradients(total_loss, kernel9)))\n",
    "    \n",
    "#     do1 = tf.reduce_mean(tf.abs(tf.gradients(total_loss, conv1)))\n",
    "#     do5 = tf.reduce_mean(tf.abs(tf.gradients(total_loss, conv5)))\n",
    "#     do9 = tf.reduce_mean(tf.abs(tf.gradients(total_loss, conv9)))\n",
    "\n",
    "\n",
    "    trainer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(total_loss, global_step = global_step)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.cast(tf.arg_max(y_out, 1), tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return (x, y, is_training, keep_prob), trainer, accuracy, total_loss, y_out, tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TRAIN NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(is_using_BN = True,\n",
    "          is_continue = False,\n",
    "          model_name = None,\n",
    "          current_step = 0,\n",
    "          max_steps = 50000,\n",
    "          eval_interval = 200,\n",
    "          save_interval = 5000):\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "            (x_train, y_train, is_training, keep_prob), trainer, accuracy, loss, y_out, saver \\\n",
    "            = build_graph(is_using_BN = is_using_BN, is_using_dropout = True)\n",
    "    \n",
    "    if is_continue:\n",
    "        \"\"\"load something\"\"\"\n",
    "        f = open(os.path.join(config.store_path, \"losses.bin\"), \"rb\")\n",
    "        losses = np.fromfile(f, dtype = np.float32)\n",
    "        f.close()\n",
    "        f = open(os.path.join(config.store_path, \"train_acc.bin\"), \"rb\")\n",
    "        train_acc = np.fromfile(f, dtype = np.float32)\n",
    "        f.close()\n",
    "        f = open(os.path.join(config.store_path, \"test_acc.bin\"), \"rb\")\n",
    "        test_acc = np.fromfile(f, dtype = np.float32)\n",
    "        f.close()\n",
    "    else:\n",
    "        losses, train_acc, test_acc = np.array([], dtype = np.float32), np.array([], dtype = np.float32), np.array([], dtype = np.float32)\n",
    "    \n",
    "    def evaluate(sess, num_examples = 10000, is_on_training_set = True): \n",
    "        accs, ls = [], []\n",
    "        if is_on_training_set:\n",
    "            prem = np.random.permutation(dataset.num_train_images)\n",
    "            x = dataset.x_train[prem[:num_examples]]\n",
    "            y = dataset.y_train[prem[:num_examples]]\n",
    "        else:\n",
    "            x = dataset.x_test[:num_examples]\n",
    "            y = dataset.y_test[:num_examples]\n",
    "        \n",
    "        for i in range(0, num_examples, 1000):\n",
    "            res = sess.run([accuracy, loss],\n",
    "                           feed_dict = {x_train: x[i : i + 1000],\n",
    "                                        y_train: y[i : i + 1000], \n",
    "                                        is_training: False,\n",
    "                                       keep_prob : 1})\n",
    "            accs.append(res[0])\n",
    "            ls.append(res[1])\n",
    "        \n",
    "        return sum(accs) / len(accs), sum(ls) / len(ls)\n",
    "\n",
    "    with tf.Session(graph = graph) as sess:\n",
    "        if is_continue:\n",
    "            if model_name is None:\n",
    "                raise ValueError('Need the name of model')\n",
    "            saver = tf.train.import_meta_graph(os.path.join(config.store_path, model_name + '.meta'))\n",
    "            saver.restore(sess, os.path.join(config.store_path, model_name))\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        def save(arr, name):\n",
    "            f = open(os.path.join(config.store_path, name+ \".bin\"), \"wb\")\n",
    "            arr.astype(np.float32).tofile(f)\n",
    "            f.close()\n",
    "        \n",
    "        for i in tqdm.tqdm(range(current_step + 1, max_steps + 1)):\n",
    "            batch = dataset.get_next_batch()\n",
    "            _,l = sess.run([trainer, loss], \\\n",
    "                            feed_dict = {x_train : batch[0],\n",
    "                                         y_train : batch[1],\n",
    "                                         is_training : True,\n",
    "                                         keep_prob: 0.6})\n",
    "\n",
    "            if i % save_interval == 0:\n",
    "                saved_model = saver.save(sess, config.store_path + 'model.ckpt', i)\n",
    "                save(losses, 'losses')\n",
    "                save(train_acc, 'train_acc')\n",
    "                save(test_acc, 'test_acc')\n",
    "                \n",
    "            \n",
    "            if i % eval_interval == 0:\n",
    "                test_acc = np.append(test_acc, evaluate(sess, 10000, False)[0]).astype(np.float32)\n",
    "                res = evaluate(sess)\n",
    "                train_acc = np.append(train_acc, res[0])\n",
    "                losses = np.append(losses, res[1])\n",
    "                \n",
    "                displayer.clear_output()\n",
    "                print(test_acc[-1], train_acc[-1])\n",
    "                plt.figure(1)\n",
    "                plt.plot(range(0, i, eval_interval), train_acc, 'red', range(0, i, eval_interval), test_acc, 'blue')\n",
    "                plt.figure(2)\n",
    "                plt.plot(range(0, i, eval_interval), losses)\n",
    "                plt.show()\n",
    "                \n",
    "        \n",
    "    return losses, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "config.learning_rate = 0.1\n",
    "config.store_path = \"../../models/CNN/cifar10/withoutBN/\"\n",
    "losses, train_acc, test_acc = train(is_using_BN = False)\n",
    "''', is_continue = True, \n",
    "                                    model_name = 'model.ckpt-50000',\n",
    "                                    current_step = 50000)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "config.learning_rate = 0.1\n",
    "config.store_path = \"../../models/CNN/cifar10/withBN/\"\n",
    "losses, train_acc, test_acc = train(is_using_BN = True)\n",
    "''', is_continue = True, \n",
    "                                    model_name = 'model.ckpt-50000',\n",
    "                                    current_step = 50000)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## RESTORE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "correct = []\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    (x_eval, y_eval, is_training), _, accuracy, y_out, loss, saver_eval = build_graph(is_using_BN=True)\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    for i in range(0, 2000, 500):\n",
    "        saver_eval = tf.train.import_meta_graph('../../models/CNN/cifar10/withBN/model.ckpt-50000.meta')\n",
    "        saver_eval.restore(sess, \"../../models/CNN/cifar10/withBN/model.ckpt-50000\")\n",
    "\n",
    "        cor = sess.run([accuracy],\n",
    "                       feed_dict={x_eval: dataset.x_train[i:i + 500] / 255.0, \n",
    "                                  y_eval: dataset.y_train[i:i + 500], \n",
    "                                  is_training: False})\n",
    "        correct.append(cor[0])\n",
    "        \n",
    "print (sum(correct) / len(correct))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
