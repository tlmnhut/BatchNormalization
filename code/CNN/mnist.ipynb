{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## IMPORT, CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm, os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display as displayer\n",
    "import math\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import struct\n",
    "\n",
    "class Config(object):\n",
    "    pass\n",
    "config = Config()\n",
    "config.batch_size = 128\n",
    "config.learning_rate = 0.02\n",
    "config.use_float16 = False\n",
    "config.BN_epsilon = 1e-5\n",
    "config.BN_decay = 0.995\n",
    "config.data_path = '../../datasets/MNIST/'\n",
    "#mnist = input_data.read_data_sets('../../datasets/MNIST', one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "testing\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "def read_MNIST(dataset = \"training\", path = \".\"):\n",
    "    \"\"\"\n",
    "    Python function for importing the MNIST data set.  It returns an iterator\n",
    "    of 2-tuples with the first element being the label and the second element\n",
    "    being a numpy.uint8 2D array of pixel data for the given image.\n",
    "    \"\"\"\n",
    "    print (dataset)\n",
    "    if dataset is \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images.idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels.idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images.idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels.idx1-ubyte')\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    # Load everything in some numpy arrays\n",
    "    with open(fname_lbl, 'rb') as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(fname_img, 'rb') as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows, cols, 1) / 255\n",
    "\n",
    "    return img, lbl\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self):\n",
    "        self.batch_size = config.batch_size\n",
    "        self.x_train, self.y_train = read_MNIST(dataset='training', path=config.data_path)\n",
    "        self.x_test, self.y_test = read_MNIST(dataset='testing', path=config.data_path)\n",
    "        self.start_batch_index = self.end_batch_index = self.num_train_images = np.shape(self.y_train)[0]\n",
    "        \n",
    "    def get_next_batch(self):\n",
    "        if self.start_batch_index == self.num_train_images:\n",
    "            #suffle\n",
    "            self.suffle()\n",
    "            self.start_batch_index = 0\n",
    "            self.end_batch_index = self.batch_size\n",
    "            \n",
    "        if self.end_batch_index > self.num_train_images:\n",
    "            #suffle\n",
    "            self.end_batch_index = self.num_train_images\n",
    "            self.start_batch_index = self.num_train_images - self.batch_size\n",
    "        \n",
    "        batch_x = self.x_train[self.start_batch_index : self.end_batch_index].astype(\n",
    "            np.float16 if config.use_float16 else np.float32)\n",
    "        batch_y = self.y_train[self.start_batch_index : self.end_batch_index]\n",
    "        self.start_batch_index = self.end_batch_index\n",
    "        self.end_batch_index += self.batch_size\n",
    "        \n",
    "        return (batch_x, batch_y)\n",
    "    \n",
    "    def suffle(self):\n",
    "        perm = np.random.permutation(self.num_train_images)\n",
    "        self.x_train = self.x_train[perm]\n",
    "        self.y_train = self.y_train[perm]\n",
    "        \n",
    "dataset = Dataset()\n",
    "print (np.shape(dataset.x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## BUILD GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batch_norm_wrapper(x, is_training, step):\n",
    "    \"\"\"\n",
    "    is_training: a boolean tensor\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('batch_norm') as scope:\n",
    "        gamma = tf.Variable(tf.ones([x.get_shape()[-1]]), trainable = True)\n",
    "        beta  = tf.Variable(tf.zeros([x.get_shape()[-1]]), trainable = True)\n",
    "        pop_mean = tf.Variable(tf.zeros([x.get_shape()[-1]]), trainable=False, name = \"pop_mean\")\n",
    "        pop_var  = tf.Variable(tf.constant(1.0, shape = [x.get_shape()[-1]]), trainable=False, name = \"pop_var\")\n",
    "\n",
    "    def using_batch_statistics():\n",
    "        batch_mean, batch_var = tf.nn.moments(x,[0])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * config.BN_decay + batch_mean * (1 - config.BN_decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * config.BN_decay + batch_var * (1 - config.BN_decay))\n",
    "#         mean = tf.clip_by_value(batch_mean, pop_mean / 3, pop_mean * 3)\n",
    "#         var = tf.clip_by_value(batch_var, pop_var - 2, pop_var + 2)\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "#             return tf.cond(tf.greater(step, 5000), lambda: tf.nn.batch_normalization(x, mean, var, beta, gamma, config.BN_epsilon),\n",
    "#                                                 lambda: tf.nn.batch_normalization(x, batch_mean, batch_var, beta, gamma, config.BN_epsilon))\n",
    "            return tf.nn.batch_normalization(x,\n",
    "                batch_mean, batch_var, beta, gamma, config.BN_epsilon)\n",
    "    def using_global_statistics():\n",
    "        return tf.nn.batch_normalization(x,\n",
    "            pop_mean, pop_var, beta, gamma, config.BN_epsilon)\n",
    "    \n",
    "    return tf.cond(is_training, \n",
    "                   using_batch_statistics, \n",
    "                   using_global_statistics)\n",
    "\n",
    "\n",
    "def batch_norm_wrapper_cnn(x, n_out, is_training, step):\n",
    "    with tf.variable_scope('batch_norm'):\n",
    "        xs = x.get_shape()\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
    "                                    name = 'beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
    "                                     name = 'gamma', trainable=True)\n",
    "        pop_mean = tf.Variable(tf.zeros([n_out]), trainable=False, name = \"pop_mean\")\n",
    "        pop_var  = tf.Variable(tf.constant(1.0, shape = [n_out]), trainable=False, name = \"pop_var\")\n",
    "        \n",
    "    def using_batch_statistics():\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * config.BN_decay + batch_mean * (1 - config.BN_decay))\n",
    "        train_var  = tf.assign(pop_var,\n",
    "                               pop_var * config.BN_decay + batch_var * (1 - config.BN_decay))\n",
    "#         mean = tf.clip_by_value(batch_mean, pop_mean / 3, pop_mean * 3)\n",
    "#         var = tf.clip_by_value(batch_var, pop_var - 2, pop_var + 2)\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(x, batch_mean, batch_var, beta, gamma, config.BN_epsilon)\n",
    "#             return tf.cond(tf.greater(step, 5000), lambda: tf.nn.batch_normalization(x, mean, var, beta, gamma, config.BN_epsilon),\n",
    "#                                                 lambda: tf.nn.batch_normalization(x, batch_mean, batch_var, beta, gamma, config.BN_epsilon))\n",
    "        \n",
    "    def using_global_statistics():\n",
    "        return tf.nn.batch_normalization(x,\n",
    "            pop_mean, pop_var, beta, gamma, config.BN_epsilon)\n",
    "    \n",
    "    return tf.cond(is_training, \n",
    "                   using_batch_statistics, \n",
    "                   using_global_statistics)\n",
    "\n",
    "def variable_on_cpu(name, shape, initializer):\n",
    "    with tf.device('/gpu:0'):\n",
    "        dtype = tf.float16 if config.use_float16 else tf.float32\n",
    "        var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "    return var\n",
    "\n",
    "def variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    dtype = tf.float16 if config.use_float16 else tf.float32\n",
    "    var = variable_on_cpu(\n",
    "        name,\n",
    "        shape,\n",
    "        tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_graph(is_using_BN = True, is_using_dropout = False):\n",
    "    \"\"\"define place holder\"\"\"\n",
    "    dtype = tf.float16 if config.use_float16 else tf.float32\n",
    "    x = tf.placeholder(dtype, shape=[None, 28, 28, 1])\n",
    "    y = tf.placeholder(tf.int32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    weight_decay = 0.001\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    #xx = tf.image.resize_images(x, [32, 32])\n",
    "    \n",
    "    \"\"\"1st convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv1_1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 1, 32], \n",
    "                                             stddev = np.sqrt(1 / (3 * 3 * 1)), \n",
    "                                             wd = weight_decay)\n",
    "        conv1_1 = tf.nn.conv2d(x, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv1_1 = batch_norm_wrapper_cnn(conv1_1, 32, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 32, tf.constant_initializer(0.01))\n",
    "            conv1_1 = tf.add(conv1_1, biases)\n",
    "        conv1_1 = tf.nn.relu(conv1_1)\n",
    "        \n",
    "    with tf.variable_scope('conv1_2') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 32, 32], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 32)), \n",
    "                                             wd = weight_decay)\n",
    "        conv1_2 = tf.nn.conv2d(conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv1_2 = batch_norm_wrapper_cnn(conv1_2, 32, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 32, tf.constant_initializer(0.01))\n",
    "            conv1_2 = tf.add(conv1_2, biases)\n",
    "        conv1_2 = tf.nn.relu(conv1_2)\n",
    "    \n",
    "    \n",
    "    \"\"\"2nd convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv1_3') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 32, 32], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 3)), \n",
    "                                             wd = weight_decay)\n",
    "        conv1_3 = tf.nn.conv2d(conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv1_3 = batch_norm_wrapper_cnn(conv1_3, 32, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 32, tf.constant_initializer(0.01))\n",
    "            conv1_3 = tf.add(conv1_3, biases)\n",
    "        conv1_3 = tf.nn.relu(conv1_3)\n",
    "    \n",
    "    \n",
    "    \"\"\"3rd convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv1_4') as scope:\n",
    "        kernel = variable_with_weight_decay('weights',\n",
    "                                             shape = [3, 3, 32, 32],\n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 32)),\n",
    "                                             wd = weight_decay)\n",
    "        \n",
    "        conv1_4 = tf.nn.conv2d(conv1_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv1_4 = batch_norm_wrapper_cnn(conv1_4, 32, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 32, tf.constant_initializer(0.01))\n",
    "            conv1_4 = tf.add(conv1_4, biases)\n",
    "        conv1_4 = tf.nn.relu(conv1_4)\n",
    "        pool1 = tf.nn.max_pool(conv1_4, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding = 'SAME', name = 'pool2')\n",
    "#         if is_using_BN:\n",
    "#             pool1 = batch_norm_wrapper_cnn(pool1, 32, is_training)\n",
    "        \n",
    "    \n",
    "    \"\"\"1st convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv2_1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 32, 64], \n",
    "                                             stddev = np.sqrt(1 / (3 * 3 * 64)), \n",
    "                                             wd = weight_decay)\n",
    "        conv2_1 = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv2_1 = batch_norm_wrapper_cnn(conv2_1, 64, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 64, tf.constant_initializer(0.01))\n",
    "            conv2_1 = tf.add(conv2_1, biases)\n",
    "        conv2_1 = tf.nn.relu(conv2_1)\n",
    "    \n",
    "    \n",
    "    \"\"\"2nd convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv2_2') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 64, 64], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 64)), \n",
    "                                             wd = weight_decay)\n",
    "        conv2_2 = tf.nn.conv2d(conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv2_2 = batch_norm_wrapper_cnn(conv2_2, 64, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 64, tf.constant_initializer(0.01))\n",
    "            conv2_2 = tf.add(conv2_2, biases)\n",
    "        conv2_2 = tf.nn.relu(conv2_2)\n",
    "        \n",
    "    with tf.variable_scope('conv2_3') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 64, 64], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 64)), \n",
    "                                             wd = weight_decay)\n",
    "        conv2_3 = tf.nn.conv2d(conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv2_3 = batch_norm_wrapper_cnn(conv2_3, 64, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 64, tf.constant_initializer(0.01))\n",
    "            conv2_3 = tf.add(conv2_3, biases)\n",
    "        conv2_3 = tf.nn.relu(conv2_3)\n",
    "    \n",
    "    \n",
    "    \"\"\"3rd convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv2_4') as scope:\n",
    "        kernel = variable_with_weight_decay('weights',\n",
    "                                             shape = [3, 3, 64, 64],\n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 64)),\n",
    "                                             wd = weight_decay)\n",
    "        \n",
    "        conv2_4 = tf.nn.conv2d(conv2_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv2_4 = batch_norm_wrapper_cnn(conv2_4, 64, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 64, tf.constant_initializer(0.01))\n",
    "            conv2_4 = tf.add(conv2_4, biases)\n",
    "        conv2_4 = tf.nn.relu(conv2_4)\n",
    "        \n",
    "        pool2 = tf.nn.max_pool(conv2_4, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding = 'SAME', name = 'pool2')\n",
    "#         if is_using_BN:\n",
    "#             pool2 = batch_norm_wrapper_cnn(pool2, 64, is_training)\n",
    "        \n",
    "    \n",
    "    with tf.variable_scope('conv3_1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 64, 128], \n",
    "                                             stddev = np.sqrt(1 / (3 * 3 * 64)), \n",
    "                                             wd = weight_decay)\n",
    "        conv3_1 = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv3_1 = batch_norm_wrapper_cnn(conv3_1, 128, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 128, tf.constant_initializer(0.01))\n",
    "            conv3_1 = tf.add(conv3_1, biases)\n",
    "        conv3_1 = tf.nn.relu(conv3_1)\n",
    "    \n",
    "    \n",
    "    \"\"\"2nd convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv3_2') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 128, 128], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 128)), \n",
    "                                             wd = weight_decay)\n",
    "        conv3_2 = tf.nn.conv2d(conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv3_2 = batch_norm_wrapper_cnn(conv3_2, 128, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 128, tf.constant_initializer(0.01))\n",
    "            conv3_2 = tf.add(conv3_2, biases)\n",
    "        conv3_2 = tf.nn.relu(conv3_2)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('conv3_3') as scope:\n",
    "        kernel = variable_with_weight_decay('weights', \n",
    "                                             shape = [3, 3, 128, 128], \n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 128)), \n",
    "                                             wd = weight_decay)\n",
    "        conv3_3 = tf.nn.conv2d(conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv3_3 = batch_norm_wrapper_cnn(conv3_3, 128, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 128, tf.constant_initializer(0.01))\n",
    "            conv3_3 = tf.add(conv3_3, biases)\n",
    "        conv3_3 = tf.nn.relu(conv3_3)\n",
    "    \n",
    "    \n",
    "    \"\"\"3rd convolution layer with max pooling\"\"\"\n",
    "    with tf.variable_scope('conv3_4') as scope:\n",
    "        kernel = variable_with_weight_decay('weights',\n",
    "                                             shape = [3, 3, 128, 128],\n",
    "                                             stddev = np.sqrt(2 / (3 * 3 * 128)),\n",
    "                                             wd = weight_decay)\n",
    "        \n",
    "        conv3_4 = tf.nn.conv2d(conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        if is_using_BN:\n",
    "            conv3_4 = batch_norm_wrapper_cnn(conv3_4, 128, is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', 128, tf.constant_initializer(0.01))\n",
    "            conv3_4 = tf.add(conv3_4, biases)\n",
    "        conv3_4 = tf.nn.relu(conv3_4)\n",
    "        \n",
    "        pool3 = tf.nn.avg_pool(conv3_4, ksize=[1, 4, 4, 1], strides=[1, 3, 3, 1],\n",
    "                         padding = 'VALID', name = 'pool9')\n",
    "#         if is_using_BN:\n",
    "#             pool3 = batch_norm_wrapper_cnn(pool3, 128, is_training)\n",
    "            \n",
    "        pool3_flat = tf.reshape(pool3, [-1, 512])\n",
    "    \n",
    "    \n",
    "#     \"\"\"1st full connection layer\"\"\"\n",
    "    with tf.variable_scope('fc1') as scope:\n",
    "#         pool4_flat = tf.reshape(pool4, [-1, 2048])\n",
    "#         if is_using_dropout:\n",
    "#             pool4_flat = tf.contrib.layers.dropout(pool4_flat, keep_prob = 0.8, is_training = is_training)\n",
    "        \n",
    "        weights = variable_with_weight_decay('weights', \n",
    "                                             shape = [512, 512],\n",
    "                                             stddev = np.sqrt(2 / (512)), \n",
    "                                             wd = weight_decay)\n",
    "        if is_using_BN:\n",
    "            fc1_logits = batch_norm_wrapper(tf.matmul(pool3_flat, weights), is_training = is_training, step = global_step)\n",
    "        else:\n",
    "            biases = variable_on_cpu('biases', [512], tf.constant_initializer(0.01))\n",
    "            fc1_logits = tf.add(tf.matmul(pool3_flat, weights), \n",
    "                                         biases, name = \"logits\")\n",
    "        fc1_out = tf.nn.relu(fc1_logits, name = \"after_activation\")\n",
    "        if is_using_dropout:\n",
    "            fc1_out = tf.contrib.layers.dropout(fc1_out, keep_prob = 0.6, is_training = is_training)\n",
    "    \n",
    "\n",
    "    \n",
    "    \"\"\"last full connection layer\"\"\"\n",
    "    with tf.variable_scope('softmax1') as scope:\n",
    "        weights = variable_with_weight_decay(name = 'weights', \n",
    "                                             shape = [512, 10],\n",
    "                                             stddev = np.sqrt(2 /(512)), \n",
    "                                             wd = weight_decay)\n",
    "        biases = variable_on_cpu('biases', [10],\n",
    "                              tf.constant_initializer(0.01))\n",
    "        y_out = tf.add(tf.matmul(fc1_out, weights), biases, name = \"output\")\n",
    "        \n",
    "    \n",
    "    \"\"\"Loss, Optimizer and Predictions\"\"\"\n",
    "    #y = tf.cast(y, tf.int64)\n",
    "    cross_entropy1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=y_out, name='cross_entropy'))\n",
    "    tf.add_to_collection('losses', cross_entropy1)\n",
    "    \n",
    "    total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "    \n",
    "    learning_rate = tf.maximum(tf.train.exponential_decay(config.learning_rate, global_step,\n",
    "                                           1000, 0.95, staircase=True), 0.01)\n",
    "\n",
    "    trainer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(total_loss, global_step = global_step)\n",
    "    #trainer = tf.train.MomentumOptimizer(learning_rate, momentum).minimize(total_loss, global_step = global_step)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.cast(tf.arg_max(y_out, 1), tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return (x, y, is_training), trainer, accuracy, total_loss, y_out, tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(is_using_BN = True,\n",
    "          is_continue = False,\n",
    "          model_name = None,\n",
    "          current_step = 0,\n",
    "          max_steps = 60000,\n",
    "          eval_interval = 200,\n",
    "          save_interval = 10000):\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        if is_using_BN:\n",
    "            (x_train, y_train, is_training), trainer, accuracy, loss, y_out, saver = build_graph(is_using_BN = True,\n",
    "                                                                                    is_using_dropout = True)\n",
    "        else:\n",
    "            (x_train, y_train, is_training), trainer, accuracy, loss, y_out, saver = build_graph(is_using_BN = False,\n",
    "                                                                                    is_using_dropout = True)\n",
    "    \n",
    "    if is_continue:\n",
    "        \"\"\"load something\"\"\"\n",
    "        f = open(os.path.join(config.store_path, \"losses.bin\"), \"rb\")\n",
    "        losses = np.fromfile(f, dtype = np.float32)\n",
    "        f.close()\n",
    "        f = open(os.path.join(config.store_path, \"train_acc.bin\"), \"rb\")\n",
    "        train_acc = np.fromfile(f, dtype = np.float32)\n",
    "        f.close()\n",
    "        f = open(os.path.join(config.store_path, \"test_acc.bin\"), \"rb\")\n",
    "        test_acc = np.fromfile(f, dtype = np.float32)\n",
    "        f.close()\n",
    "    else:\n",
    "        losses, train_acc, test_acc = np.array([], dtype = np.float32), np.array([], dtype = np.float32), np.array([], dtype = np.float32)\n",
    "    \n",
    "    def evaluate(sess, num_examples = 10000, is_on_training_set = True): \n",
    "        accs = []\n",
    "        if is_on_training_set:\n",
    "            prem = np.random.permutation(60000)\n",
    "            x = dataset.x_train[prem[:num_examples]]\n",
    "            y = dataset.y_train[prem[:num_examples]]\n",
    "        else:\n",
    "            x = dataset.x_test[:num_examples]\n",
    "            y = dataset.y_test[:num_examples]\n",
    "        \n",
    "        for i in range(0, num_examples, 500):\n",
    "            res = sess.run([accuracy],\n",
    "                           feed_dict = {x_train: x[i : i + 500],\n",
    "                                        y_train: y[i : i + 500], \n",
    "                                        is_training: False})\n",
    "            accs.append(res[0])\n",
    "        \n",
    "        return sum(accs) / len(accs)\n",
    "\n",
    "#     popvar_FC1 = []\n",
    "#     popmean_FC1 = []\n",
    "    with tf.Session(graph = graph) as sess:\n",
    "        if is_continue:\n",
    "            if model_name is None:\n",
    "                raise ValueError('Need the name of model')\n",
    "            saver = tf.train.import_meta_graph(os.path.join(config.store_path, model_name + '.meta'))\n",
    "            saver.restore(sess, os.path.join(config.store_path, model_name))\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        stop = 0    \n",
    "        for i in tqdm.tqdm(range(current_step + 1, max_steps + 1)):\n",
    "            batch = dataset.get_next_batch()\n",
    "            _,l, acc = sess.run([trainer, loss, accuracy], feed_dict = {x_train : batch[0],\n",
    "                                             y_train : batch[1],\n",
    "                                             is_training : True})\n",
    "            #losses.append(l)\n",
    "#             popvar_FC1.append(pvfc1)\n",
    "#             popmean_FC1.append(pmfc1)\n",
    "            if i % save_interval == 0:\n",
    "                saved_model = saver.save(sess, config.store_path + 'model.ckpt', i)\n",
    "                f = open(os.path.join(config.store_path, \"losses.bin\"), \"wb\")\n",
    "                losses.tofile(f)\n",
    "                f.close()\n",
    "                f = open(os.path.join(config.store_path, \"train_acc.bin\"), \"wb\")\n",
    "                train_acc.tofile(f)\n",
    "                f.close()\n",
    "                f = open(os.path.join(config.store_path, \"test_acc.bin\"), \"wb\")\n",
    "                test_acc.tofile(f)\n",
    "                f.close()\n",
    "                \n",
    "            \n",
    "            if i % eval_interval == 0:\n",
    "                losses = np.append(losses, l)\n",
    "                test_acc = np.append(test_acc, evaluate(sess, 10000, False)).astype(np.float32)\n",
    "                train_acc = np.append(train_acc, evaluate(sess)).astype(np.float32)\n",
    "                \n",
    "                \n",
    "                displayer.clear_output()\n",
    "                print(test_acc[-1])\n",
    "                print(train_acc[-1])\n",
    "                print(l)\n",
    "                plt.figure(1)\n",
    "                plt.plot(range(0, i, eval_interval), train_acc, 'red', range(0, i, eval_interval), test_acc, 'blue')\n",
    "                plt.figure(2)\n",
    "                plt.plot(range(0, i, eval_interval), losses)\n",
    "#                 ax1 = plt.subplot(111)\n",
    "#                 #plt.plot(np.arange(0, i, eval_interval), losses)\n",
    "#                 #plt.figure(1)\n",
    "#                 ax1.plot(range(0, i, eval_interval), train_acc, 'red', range(0, i, eval_interval), test_acc, 'blue')\n",
    "#                 plt.figure(2)\n",
    "#                 plt.plot(np.arange(0, i), popvar_FC1)\n",
    "#                 plt.figure(3)\n",
    "#                 plt.plot(np.arange(0, i), popmean_FC1)\n",
    "                plt.show()\n",
    "                \n",
    "        \n",
    "    return losses, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36\n",
      "0.36\n",
      "3.51247\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE4BJREFUeJzt3X+snmd93/H3J+dgfnjKgOaky+xkNtnhD7MgA48SpjUp\nhYQ6C03ctVWTWcHaNFlmtiItQo0nUql4/WP1NDMhLCIzBYqKcbcihCVgHlQLW6Ro82MwTZzN87FL\nlRNH6wlYS7K0JA7f/XEui9v2E5/n/PaB90u69dz3dX3v61xXjvR8fP84SqoKSZKuWu4JSJKuDAaC\nJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1o8s9gdm45pprat26dcs9DUlaUY4ePfp8\nVY3NVLeiAmHdunX0+/3lnoYkrShJ/mKYOm8ZSZIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIM\nBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUG\ngiQJGDIQkmxKciLJRJJdA/q3J3kyybEkjyfZ0Nq3tLbz20+SbGx9j7Uxz/ddu7BLkyTNxuhMBUlG\ngH3AHcAkcCTJoap6ulN2oKoeafV3A3uBTVX1JeBLrf0m4GtVdaxz3paq6i/MUiRJ8zHMFcLNwERV\nna6qV4CDwD3dgqp6oXO4GqgB49wHfHmuE5UkLa4ZrxCANcAzneNJ4JaLi5LsAB4EVgEfHDDOb3NR\nkACfT/Ia8BXg96tqUJBIkpbAMFcIGdB2yRd3Ve2rqhuBh4CHLxgguQV4uaqe6jRvqaqbgFvbdv/A\nH55sS9JP0p+amhpiupKkuRgmECaB6zvHa4Ezl6k/CGy+qO1eLrpdVFXPts8XgQNM35q6RFXtr6pe\nVfXGxsaGmK4kaS6GCYQjwHiS9UlWMf3lfqhbkGS8c3gXcLLTdxXwW0wHxfm20STXtP03AB8BulcP\nkqQlNuMzhKo6l2QncBgYAR6tquNJdgP9qjoE7ExyO/AqcBbY2hniNmCyqk532t4IHG5hMAJ8G/jc\ngqxIkjQnWUnPcXu9XvX7vqUqSbOR5GhV9Waq8y+VJUmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQ\nJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSMGQg\nJNmU5ESSiSS7BvRvT/JkkmNJHk+yobVvaW3nt58k2dj63tfOmUjy6SRZ2KVJkmZjxkBIMgLsA+4E\nNgD3nf/C7zhQVTdV1UZgD7AXoKq+VFUbW/v9wA+q6lg757PANmC8bZsWYkGSpLkZ5grhZmCiqk5X\n1SvAQeCebkFVvdA5XA3UgHHuA74MkOQ64OqqeqKqCvgisHkO85ckLZDRIWrWAM90jieBWy4uSrID\neBBYBXxwwDi/zU+DZE0bpzvmmkE/PMk2pq8kuOGGG4aYriRpLoa5Qhh0b/+SK4Cq2ldVNwIPAQ9f\nMEByC/ByVT01mzHbuPurqldVvbGxsSGmK0mai2ECYRK4vnO8FjhzmfqDXHr7517a7aLOmGtnMaYk\naZENEwhHgPEk65OsYvrL/VC3IMl45/Au4GSn7yrgt5gOCgCq6jngxSTvb28XfRT42pxXIUmatxmf\nIVTVuSQ7gcPACPBoVR1PshvoV9UhYGeS24FXgbPA1s4QtwGTVXX6oqE/BnwBeDPwzbZJkpZJpl/y\nWRl6vV71+/3lnoYkrShJjlZVb6Y6/1JZkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIa\nA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpGaoQEiyKcmJJBNJdg3o\n357kySTHkjyeZEOn791JnkhyvNW8qbU/1sY81rZrF25ZkqTZGp2pIMkIsA+4A5gEjiQ5VFVPd8oO\nVNUjrf5uYC+wKcko8EfA/VX1/SS/ALzaOW9LVfk/SZakK8AwVwg3AxNVdbqqXgEOAvd0C6rqhc7h\naqDa/oeBP6uq77e6H1bVa/OftiRpoQ0TCGuAZzrHk63tAkl2JDkF7AEeaM3vBCrJ4STfTfI7F532\n+Xa76HeTZA7zlyQtkGECYdAXdV3SULWvqm4EHgIebs2jwC8BW9rnryf5UOvbUlU3Abe27f6BPzzZ\nlqSfpD81NTXEdCVJczFMIEwC13eO1wJnLlN/ENjcOfc7VfV8Vb0MfAN4L0BVPds+XwQOMH1r6hJV\ntb+qelXVGxsbG2K6kqS5GCYQjgDjSdYnWQXcCxzqFiQZ7xzeBZxs+4eBdyd5S3vA/MvA00lGk1zT\nzn0D8BHgqfktRZI0HzO+ZVRV55LsZPrLfQR4tKqOJ9kN9KvqELAzye1Mv0F0Ftjazj2bZC/ToVLA\nN6rq60lWA4dbGIwA3wY+twjrkyQNKVWXPA64YvV6ver3fUtVkmYjydGq6s1U518qS5IAA0GS1BgI\nkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwE\nSVJjIEiSAANBktQMFQhJNiU5kWQiya4B/duTPJnkWJLHk2zo9L07yRNJjreaN7X297XjiSSfTpKF\nW5YkabZmDIQkI8A+4E5gA3Bf9wu/OVBVN1XVRmAPsLedOwr8EbC9qt4FfAB4tZ3zWWAbMN62TfNe\njSRpzoa5QrgZmKiq01X1CnAQuKdbUFUvdA5XA9X2Pwz8WVV9v9X9sKpeS3IdcHVVPVFVBXwR2DzP\ntUiS5mGYQFgDPNM5nmxtF0iyI8kppq8QHmjN7wQqyeEk303yO50xJ2caU5K0dIYJhEH39uuShqp9\nVXUj8BDwcGseBX4J2NI+fz3Jh4YdEyDJtiT9JP2pqakhpitJmothAmESuL5zvBY4c5n6g/z09s8k\n8J2qer6qXga+Aby3ta8dZsyq2l9VvarqjY2NDTFdSdJcDBMIR4DxJOuTrALuBQ51C5KMdw7vAk62\n/cPAu5O8pT1g/mXg6ap6Dngxyfvb20UfBb42z7VIkuZhdKaCqjqXZCfTX+4jwKNVdTzJbqBfVYeA\nnUluZ/oNorPA1nbu2SR7mQ6VAr5RVV9vQ38M+ALwZuCbbZMkLZNMv+SzMvR6ver3+8s9DUlaUZIc\nrareTHX+pbIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJ\nUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQKGDIQkm5KcSDKRZNeA/u1JnkxyLMnjSTa0\n9nVJ/qq1H0vySOecx9qY5/uuXbhlSZJma3SmgiQjwD7gDmASOJLkUFU93Sk7UFWPtPq7gb3AptZ3\nqqo2vs7wW6qqP+fZS5IWzDBXCDcDE1V1uqpeAQ4C93QLquqFzuFqoBZuipKkpTBMIKwBnukcT7a2\nCyTZkeQUsAd4oNO1Psn3knwnya0Xnfb5drvod5NktpOXJC2cYQJh0Bf1JVcAVbWvqm4EHgIebs3P\nATdU1XuAB4EDSa5ufVuq6ibg1rbdP/CHJ9uS9JP0p6amhpiuJGkuhgmESeD6zvFa4Mxl6g8CmwGq\n6sdV9cO2fxQ4BbyzHT/bPl8EDjB9a+oSVbW/qnpV1RsbGxtiupKkuRgmEI4A40nWJ1kF3Asc6hYk\nGe8c3gWcbO1j7aE0Sd4BjAOnk4wmuaa1vwH4CPDUfBcjSZq7Gd8yqqpzSXYCh4ER4NGqOp5kN9Cv\nqkPAziS3A68CZ4Gt7fTbgN1JzgGvAdur6kdJVgOHWxiMAN8GPrfQi5MkDS9VK+eFoF6vV/2+b6lK\n0mwkOVpVvZnq/EtlSRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GS\nBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkpqhAiHJpiQnkkwk2TWgf3uSJ5McS/J4kg2t\nfV2Sv2rtx5I80jnnfe2ciSSfTpKFW5YkabZmDIQkI8A+4E5gA3Df+S/8jgNVdVNVbQT2AHs7faeq\namPbtnfaPwtsA8bbtmke65AkzdMwVwg3AxNVdbqqXgEOAvd0C6rqhc7haqAuN2CS64Crq+qJqirg\ni8DmWc1ckrSghgmENcAznePJ1naBJDuSnGL6CuGBTtf6JN9L8p0kt3bGnJxpTEnS0hkmEAbd27/k\nCqCq9lXVjcBDwMOt+Tnghqp6D/AgcCDJ1cOOCZBkW5J+kv7U1NQQ05UkzcUwgTAJXN85XgucuUz9\nQdrtn6r6cVX9sO0fBU4B72xjrh1mzKraX1W9quqNjY0NMV1J0lwMEwhHgPEk65OsAu4FDnULkox3\nDu8CTrb2sfZQmiTvYPrh8emqeg54Mcn729tFHwW+Nu/VSJLmbHSmgqo6l2QncBgYAR6tquNJdgP9\nqjoE7ExyO/AqcBbY2k6/Ddid5BzwGrC9qn7U+j4GfAF4M/DNtkmSlkmmX/JZGXq9XvX7/eWehiSt\nKEmOVlVvpjr/UlmSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQB\nBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAFDBkKSTUlOJJlIsmtA//YkTyY5luTx\nJBsu6r8hyUtJPt5p+0HnHP9HyZK0zEZnKkgyAuwD7gAmgSNJDlXV052yA1X1SKu/G9gLbOr0fwr4\n5oDhf6Wqnp/r5CVJC2eYK4SbgYmqOl1VrwAHgXu6BVX1QudwNVDnD5JsBk4Dx+c/XUnSYhkmENYA\nz3SOJ1vbBZLsSHIK2AM80NpWAw8BnxwwbgH/OcnRJNte74cn2Zakn6Q/NTU1xHQlSXMxTCBkQFtd\n0lC1r6puZDoAHm7NnwQ+VVUvDRjjH1TVe4E7gR1Jbhv0w6tqf1X1qqo3NjY2xHQlSXMx4zMEpq8I\nru8crwXOXKb+IPDZtn8L8JtJ9gBvBX6S5K+r6jNVdQagqv4yyVeZvjX1X2e7AEnSwhgmEI4A40nW\nA88C9wL/uFuQZLyqTrbDu4CTAFV1a6fm94CXquoz7VbSVVX1Ytv/MLB7vouRJM3djIFQVeeS7AQO\nAyPAo1V1PMluoF9Vh4CdSW4HXgXOAltnGPYXga8mOT+HA1X1n+axDknSPKXqkscBV6xer1f9vn+y\nIEmzkeRoVfVmqvMvlSVJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwE\nSRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBECqarnnMLQkU8Bf\nLPc8Zuka4PnlnsQSc80/H1zzyvF3qmpspqIVFQgrUZJ+VfWWex5LyTX/fHDNP3u8ZSRJAgwESVJj\nICy+/cs9gWXgmn8+uOafMT5DkCQBXiFIkhoDYQEkeXuSbyU52T7f9jp1W1vNySRbB/QfSvLU4s94\n/uaz5iRvSfL1JP8ryfEk/3ppZz87STYlOZFkIsmuAf1vTPLHrf+/J1nX6fuXrf1Ekl9dynnPx1zX\nnOSOJEeTPNk+P7jUc5+L+fyOW/8NSV5K8vGlmvOiqCq3eW7AHmBX298F/MGAmrcDp9vn29r+2zr9\n/wg4ADy13OtZ7DUDbwF+pdWsAv4bcOdyr+l11jkCnALe0eb6fWDDRTX/HHik7d8L/HHb39Dq3wis\nb+OMLPeaFnnN7wH+dtv/e8Czy72exVxvp/8rwH8EPr7c65nP5hXCwrgH+MO2/4fA5gE1vwp8q6p+\nVFVngW8BmwCS/A3gQeD3l2CuC2XOa66ql6vqvwBU1SvAd4G1SzDnubgZmKiq022uB5lee1f3v8Wf\nAB9KktZ+sKp+XFV/Dky08a50c15zVX2vqs609uPAm5K8cUlmPXfz+R2TZDPT/9g5vkTzXTQGwsL4\nxap6DqB9XjugZg3wTOd4srUB/Cvg3wIvL+YkF9h81wxAkrcCvwb86SLNc75mXEO3pqrOAf8X+IUh\nz70SzWfNXb8BfK+qfrxI81woc15vktXAQ8Anl2Cei250uSewUiT5NvC3BnR9YtghBrRVko3A362q\nf3Hxfcnltlhr7ow/CnwZ+HRVnZ79DJfEZdcwQ80w516J5rPm6c7kXcAfAB9ewHktlvms95PAp6rq\npXbBsKIZCEOqqttfry/J/0lyXVU9l+Q64C8HlE0CH+gcrwUeA/4+8L4kP2D693Ftkseq6gMss0Vc\n83n7gZNV9e8WYLqLZRK4vnO8FjjzOjWTLeT+JvCjIc+9Es1nzSRZC3wV+GhVnVr86c7bfNZ7C/Cb\nSfYAbwV+kuSvq+oziz/tRbDcDzF+Fjbg33DhA9Y9A2reDvw50w9V39b2335RzTpWzkPlea2Z6ecl\nXwGuWu61zLDOUabvD6/npw8c33VRzQ4ufOD4H9r+u7jwofJpVsZD5fms+a2t/jeWex1Lsd6Lan6P\nFf5Qedkn8LOwMX3v9E+Bk+3z/JdeD/j3nbp/yvSDxQngnwwYZyUFwpzXzPS/wAr4n8Cxtv2z5V7T\nZdb6D4H/zfSbKJ9obbuBu9v+m5h+w2QC+B/AOzrnfqKdd4Ir9E2qhVwz8DDw/zq/12PAtcu9nsX8\nHXfGWPGB4F8qS5IA3zKSJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiQA/j8YBSqF7K8j\nVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2403c9dbc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUkMMfUlqiKEvSQ35/xj6ExzMWEPaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2403cbdcda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 24/60000 [00:59<43:20:25,  2.60s/it]"
     ]
    }
   ],
   "source": [
    "# 13.7 it/s\n",
    "config.learning_rate = 0.1\n",
    "config.store_path = \"../../models/CNN/MNIST/withoutBN/\"\n",
    "losses, train_acc, test_acc = train(is_using_BN = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9965\n",
      "1.0\n",
      "0.0172727\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHVxJREFUeJzt3X+UXGWd5/H3p6q6OyEBEpImhCQkQQNDFDTQRFF2YBg5\nE2AM486sh5wzOziimZ0xODvqzIB6GBbXWdfZWT3OMGjWZZU9SkREjZy4LLLgqiuQRn4GCLThR9oA\n6fA7Abq7qr77x73dqa6+1V2B6nTf5vM6p07fH0/f+zzpm08//dxfigjMzGx6KUx2BczMrPUc7mZm\n05DD3cxsGnK4m5lNQw53M7NpyOFuZjYNOdzNzKYhh7uZ2TTkcDczm4ZKk7Xj+fPnx7JlyyZr92Zm\nuXTXXXftiYjO8cpNWrgvW7aM7u7uydq9mVkuSXqimXIeljEzm4Yc7mZm05DD3cxsGnK4m5lNQw53\nM7NpaNxwl3S1pN2SHmiwXpK+IqlH0n2STm59Nc3M7EA003P/BrBmjPXnACvSz3rgqjdeLTMzeyPG\nvc49Iv6vpGVjFDkfuCaS9/XdLmmOpIUR8VSL6jg1DL2OUEq+DgxAqQQPPggPPwyHHgrveU/yNcvT\nT8PmzbBvH7zyChx5JJxzDuzZA089BU88Ac8+C4cdlpSJgIULoVyG117b/xkYODjtNbOJ8/73w6mn\nTuguWnET0yJgZ818b7psVLhLWk/Su+eYY45pwa7fgBdegH/6J7jwQhiqS38/tLcnAXrRRfCRj8Dt\ntydhe9VV8KlPwb33whFHwI03wrvfTf8tP2fnszOpUmDJO+Yx84eboK0NFiyAahU++1l49lme/PmT\nPL79NVZxN1s5ldnsZTXreY0OtnM8e5lND2+lRJkTuZ8O+lnKE8ygn99wNDdzNg/wdo5hN3tJfoEU\nqVCgSpEKIihTYpC24U+ZEofwCk+xEICZvMoMXqOfDmaxjyN4jjIlXuZQ9jKbvczmcF5kBq8xQDsD\ntBOIEmUqFOmnA4ASZdoYBKBMiTIl2hhkJq/STweB6KeDV5jJbPZRoMo+ZqUly4jgVWZSocARPE8g\nKhSpUqBCccT0eMZ7A3CgJrYxdcpMx7pUKQwfk0PHTjsD6df90yXK7GU2L3EoVQrpkZAc4wWqPM9c\nAjGTVylRbrg/jXFUHPx12f5k71OcObHZ3pJwz6p/ZmsjYiOwEaCrq2ti38y9dy986UtJ7/rjH4dD\nDoHubli5En75SwbWb+C7j53C+7/5Bxy26i0wOMi+G29lxnHHUH3LCj584we4aOu1zH/kF1Qo8kk2\n8SeXX8fT/Us4iqfZyLc45cm7uI1Pcx/vAOB9997MzcceC4sXw9KlcOed/Lp/EXuZzUf4Ot2M/Gle\n+K6H+MUTS+h5enZmE6Rg9qzg5b3J6FmxGFQq4/9nSspCoQCDg8nvomJxf+e/vT3546H23eizZiWf\nF15Ifre1tycfKfnjoVgcOV8uJ9/f1pb8E/f3J9vu6EjKtLXB7NnJj6FSSaYrleT7qtXkxwHw3HNJ\nPYvFkZ+hZWqiueOVacU2DmaZ6VYXKTl2SqXkGBgYSD6DgyOnBweT4+Tww5Off7WalK9Ukuk5c5Jj\n4pVXkmVZYoxUmUrrfmdV43Wt0opw7wWW1MwvBna1YLsH7uWX4fvfh/PO4/4rvs/7v/JvWcoTXHv/\n33H0aUvZ/fHP8V19kCdjCT/tuJY7WMUfPXYD5V2HcFhxH9fG9cx59CXOeOgWruffsO2Rt3MPVxHp\nqYmt/afyEocP7+4XnA7A5z4Hu34TXPXVs7n8qK9y2pPfY+GTu/jpO6/k8u3rePG1DipR5KIPB0uO\nEatXww03wLXXnsDSpfCNL8C8eXDCCckfCQ8/nBzwO3aIPXvE8uVw1lnw9reLPXuSg1waeeBXq8l/\nnra25FNIz6YMDibz9SoVePHF5HtmzUr+08Do0Sczy6mIGPcDLAMeaLDuPODHJD34dwN3NrPNU045\nJVpqYCB+cvJfxzu4O77XsS4ubPtWHFJ8NYoqx9/whbiLVTGz8GpARFuxHMuWVuKssyIgoqOjGqVS\nxAc+EHH66ckyiJjJvoCIFceW46x3vTy8/G1vi/j7v49Ytqg/zv3dV6Najdi7N2LevHR7pcGY3dEf\nEHHUURHHHRexfHlEf39rm2xmbz5AdzSRsYqx/nYAJF0LnAnMB54B/g5oS38xfFWSgH8muaLmFeBP\nI2LcJ4J1dXVFKx8c9vS/3MCJH/tXPF+YR6VaoECFP1v7FM8Ujua2mwd4y5zneHLwKG69rcBv/VbS\nM33pJdiwIRlaP/30pLfb3w//+I9w+/d6+dGvFgOwcyf07Q5OPkUsm/sCO56dg5QMOcyYkfR+Ae68\nE3p74eKLk57xjTcmvfFCIflTct68ljXXzN6kJN0VEV3jlWvmapl146wP4GMHULcJ8b3rq+yhk+5f\nVvj+D4Ovb4SLv7CYPXvghh/M4Ll9R3PNNUnYDjnsMLjmmpHb6eiAT38avlUSP/oVHNXxHIsWHcGi\nReKEE+CDH5wzPGQxu26ofPXq5POe9yTDJEcfvX/dzJkT024zsyyT9sjfVrvv4Q6OKL3Iyacezimr\n4T9+fv+VFrt2Jb30449vfnurzjkK/hZOPe4lpCMA2Latue896qgDqbmZWetNj3CP4N7dR/GOBU8h\nHT5q9cKFyedAHHdCkcWL4ff+bNnwMp9kNLO8mBbhXn3sCe6vrOSjx/+6ZdsslZL7ihzoZpZH0+LB\nYTv+1yO8wixOek/29eKvV6HgcDezfJoW4X7vj5PL6k9cs2iSa2JmNjVMi3D/+e1FZhT6OenUjsmu\nipnZlJD/cO/r46d73sZpy56iw9luZgZMg3B//se3cw/v5Myzxn/IlJnZm0Xuw/0Xd5QICpyxxncJ\nmZkNyX2473puBgBvfeskV8TMbArJfbgPDibPxmmbOS0u2Tcza4nch3s5eWcEpQ6PuZuZDcl9uA+m\n4d52SMZDy83M3qRyH+7ldFimNMPDMmZmQ3If7oPl5PkAbTM8LGNmNiT34T7Ucy+W/BAYM7MhTYW7\npDWStkvqkXRJxvqlkm6RdJ+k2yQtbn1Vsw2WRYlBP+DLzKzGuOEuqQhcCZwDrATWSVpZV+y/ANdE\nxEnAFcB/anVFGxksQxuDB2t3Zma50EzPfTXQExE7ImIA2AScX1dmJXBLOn1rxvoJUy6LEpWDtTsz\ns1xoJtwXATtr5nvTZbXuBf4wnf4AcKikg/I66MGyaJN77mZmtZoJ96zR7Kib/xRwhqS7gTOA3wDl\nURuS1kvqltTd19d3wJXNUq5ASe65m5nVaibce4ElNfOLgV21BSJiV0T864hYBXwmXfZi/YYiYmNE\ndEVEV2dn5xuo9n6D5QJtGvV7xMzsTa2ZcN8KrJC0XFI7cAGwubaApPmShrZ1KXB1a6vZmHvuZmaj\njRvuEVEGNgA3AQ8B10XENklXSFqbFjsT2C7pEWAB8PkJqu8ogxX33M3M6jV1z35EbAG21C27rGb6\neuD61latOeWKKBWqk7FrM7MpK/d3qLrnbmY2Wu7DvVwpuOduZlYn9+E+WC3QVvAJVTOzWrkPd/fc\nzcxGy324u+duZjZa7sO9XC1SKtTfMGtm9uaW+3AfrBZoK7rnbmZWK/fh7p67mdlouQ/3wSjSVvQJ\nVTOzWrkP93K1SKnonruZWa3ch7t77mZmo+U+3MtRpK3kcDczq5X7cB+MkodlzMzqTItwb2vq2ZZm\nZm8euQ/3Mu65m5nVy324D0aJtjaHu5lZrdyHe9Jzn+xamJlNLU2Fu6Q1krZL6pF0Scb6YyTdKulu\nSfdJOrf1Vc02SBttJffczcxqjRvukorAlcA5wEpgnaSVdcU+S/Ju1VUkL9D+l1ZXNEtUqlQoUWrT\nwdidmVluNNNzXw30RMSOiBgANgHn15UJ4LB0+nBgV+uq2Fj5teT1eu65m5mN1Ey4LwJ21sz3pstq\nXQ78saRekhdpX5y1IUnrJXVL6u7r63sd1R2p/OoggHvuZmZ1mgn3rOSs7yqvA74REYuBc4H/KWnU\ntiNiY0R0RURXZ2fngde2zuCrac+97Q1vysxsWmkm3HuBJTXzixk97HIRcB1ARPwSmAHMb0UFx1Lu\nT57j7p67mdlIzYT7VmCFpOWS2klOmG6uK/Mk8LsAkk4gCfc3Pu4yDvfczcyyjRvuEVEGNgA3AQ+R\nXBWzTdIVktamxT4JfFTSvcC1wIciYsLPcg6dUHXP3cxspKaeyhIRW0hOlNYuu6xm+kHgva2t2vgG\nX0uGZdraHe5mZrVyfYeqx9zNzLLlOtyHe+4dDnczs1q5Dvf9PfdcN8PMrOVynYr7e+65boaZWcvl\nOhU95m5mli3X4T7Yn7w71T13M7ORcp2KQz13h7uZ2Ui5TsXBgeQ+qVJ7rpthZtZyuU7F8kA6LDPD\nr2IyM6uV63AfGnN3z93MbKRcp+LQsIzH3M3MRsp1KpYH0kshZzT1iBwzszeNXIf74EDy1T13M7OR\ncp2KQydU3XM3Mxsp1+E+PObuq2XMzEbIdbj39yfh3n6Ie+5mZrWaCndJayRtl9Qj6ZKM9V+SdE/6\neUTSC62v6mi7nw5ElXlvnXswdmdmlhvjdnklFYErgbNJXpa9VdLm9O1LAETEX9WUvxhYNQF1HeWZ\nvgLz9SylQzoPxu7MzHKjmZ77aqAnInZExACwCTh/jPLrSN6jOuGeeb6dBe3PH4xdmZnlSjPhvgjY\nWTPfmy4bRdJSYDnwf9541cb39MuzWHDI3oOxKzOzXGkm3LMelh4Nyl4AXB8RlcwNSesldUvq7uvr\na7aODT3z2mEsOPzVN7wdM7Pppplw7wWW1MwvBnY1KHsBYwzJRMTGiOiKiK7Ozjc4Th7BM+V5LJhX\nfmPbMTObhpoJ963ACknLJbWTBPjm+kKSjgfmAr9sbRWz7X3qZV5hFkctOBh7MzPLl3HDPSLKwAbg\nJuAh4LqI2CbpCklra4quAzZFRKMhm5Z65sFnAViwuO1g7M7MLFeauvsnIrYAW+qWXVY3f3nrqjW+\np7e/CMCCZTMP5m7NzHIht3eoPrNjHwAL3nroJNfEzGzqyW247/7NIABHrjh8kmtiZjb15Dbc+weS\nKzRnHuYxdzOzerkN92o1OW9bKGZdhm9m9uaW33BPb5MqtuW2CWZmEya3yVhN3tNBoZTbJpiZTZjc\nJmMl7bkX2vyiDjOzerkN96Geu4dlzMxGy20yVivpCVUPy5iZjZLbZKxUk6tkPCxjZjZabsN9aFhG\nBV8KaWZWL9fhXqCCnO1mZqPkONyDAtXJroaZ2ZSU23CvVORwNzNrILfhngzLONzNzLLkOtyLZL6q\n1czsTS+34V6peljGzKyRpsJd0hpJ2yX1SLqkQZkPSnpQ0jZJ325tNUdLhmUOyhv9zMxyZ9zX7Ekq\nAlcCZwO9wFZJmyPiwZoyK4BLgfdGxPOSjpyoCg+pVqEoD8uYmWVppue+GuiJiB0RMQBsAs6vK/NR\n4MqIeB4gIna3tpqj+YSqmVljzYT7ImBnzXxvuqzWccBxkn4h6XZJa7I2JGm9pG5J3X19fa+vxqlK\nVRTkYRkzsyzNhHvWPaD1qVoCVgBnAuuAr0uaM+qbIjZGRFdEdHV2dh5oXUeohnvuZmaNNBPuvcCS\nmvnFwK6MMj+MiMGIeAzYThL2E6ZaFUWHu5lZpmbCfSuwQtJySe3ABcDmujI/AH4HQNJ8kmGaHa2s\naL1qFQpyuJuZZRk33COiDGwAbgIeAq6LiG2SrpC0Ni12E/CspAeBW4G/johnJ6rSMHSdu8fczcyy\njHspJEBEbAG21C27rGY6gE+kn4OiGr4U0syskdzeoVr11TJmZg3lNtw9LGNm1lhuw70a8glVM7MG\nchzuUHS4m5llym+4e8zdzKyh3IZ7JTzmbmbWSG7DvVqVh2XMzBrIb7iHh2XMzBrJbbgnT4V0z93M\nLEtuwz3puU92LczMpqZch7vH3M3MsuU43PGYu5lZA7kN90oUHO5mZg3kNtyrIYoFD8uYmWXJdbi7\n525mli234V6peljGzKyRpsJd0hpJ2yX1SLokY/2HJPVJuif9fKT1VR2pii+FNDNrZNw3MUkqAlcC\nZ5O8CHurpM0R8WBd0e9ExIYJqGMmj7mbmTXWTM99NdATETsiYgDYBJw/sdUan8fczcwaaybcFwE7\na+Z702X1/lDSfZKul7SkJbUbQ3Ip5ETvxcwsn5oJ96wIre8y/whYFhEnAT8Bvpm5IWm9pG5J3X19\nfQdW0zrVKFAsuOduZpalmXDvBWp74ouBXbUFIuLZiOhPZ/8bcErWhiJiY0R0RURXZ2fn66nvsOSE\nqsPdzCxLM+G+FVghabmkduACYHNtAUkLa2bXAg+1rorZKlGg4J67mVmmca+WiYiypA3ATUARuDoi\ntkm6AuiOiM3AxyWtBcrAc8CHJrDOgJ8KaWY2lnHDHSAitgBb6pZdVjN9KXBpa6s2No+5m5k1lts7\nVKvIwzJmZg3kNtx9KaSZWWO5DfcqHpYxM2skv+EeHpYxM2skt+FeoehhGTOzBnIb7knPfbJrYWY2\nNeU2HqsUKBY9LGNmliXX4e5hGTOzbLkN9+TxA5NdCzOzqSm38ehLIc3MGst1uLvnbmaWLbfxWKHo\ncDczayC38eieu5lZY7mNx+RSyMmuhZnZ1JTrcHfP3cwsWy7jMapBONzNzBrKZTxWy1UAXwppZtZA\nU+EuaY2k7ZJ6JF0yRrk/khSSulpXxdGGwr1Q9C2qZmZZxg13SUXgSuAcYCWwTtLKjHKHAh8H7mh1\nJetVBtNwz+XfHWZmE6+ZeFwN9ETEjogYADYB52eU+xzwReC1FtYv03DP3eFuZpapmXhcBOysme9N\nlw2TtApYEhE3jrUhSesldUvq7uvrO+DKDqkOVgB8KaSZWQPNhHvWwPbwmUxJBeBLwCfH21BEbIyI\nrojo6uzsbL6WdfaPub/uTZiZTWvNhHsvsKRmfjGwq2b+UODtwG2SHgfeDWyeyJOqlXLyu6XgZ/6a\nmWVqJty3AiskLZfUDlwAbB5aGREvRsT8iFgWEcuA24G1EdE9ITWmdljGl0KamWUZN9wjogxsAG4C\nHgKui4htkq6QtHaiK5hl/wlV99zNzLKUmikUEVuALXXLLmtQ9sw3Xq2xVSvpsIyvczczy5TLiwl9\nnbuZ2dhyGY/Djx/w1TJmZplyHe4eljEzy5bLcB++FNLhbmaWKZfh7mEZM7Ox5TrcfULVzCxbLuNx\nONxLuay+mdmEy2U67n/8wCRXxMxsisplPA6PuTd1C5aZ2ZtPPsO94geHmZmNJZfhPjws4zF3M7NM\nuUzHasWXQpqZjSWf4e6bmMzMxpTTcPfjB8zMxpLLcK/4kb9mZmPKZbgPDcsUSw53M7MsTYW7pDWS\ntkvqkXRJxvp/J+l+SfdI+rmkla2v6n5+WYeZ2djGDXdJReBK4BxgJbAuI7y/HREnRsQ7gS8C/7Xl\nNa3hSyHNzMbWTDquBnoiYkdEDACbgPNrC0TESzWzs4AJfXP1UM/dl0KamWVr5gb+RcDOmvle4F31\nhSR9DPgE0A6c1ZLaNeCrZczMxtZMzz0rQUf1zCPiyoh4C/C3wGczNyStl9Qtqbuvr+/AalpjeMzd\nJ1TNzDI1E+69wJKa+cXArjHKbwL+IGtFRGyMiK6I6Ors7Gy+lrUefZTKbT8DoFD0mLuZWZZm0nEr\nsELSckntwAXA5toCklbUzJ4HPNq6Ko608dLHOO8nfwX4Ukgzs0bGHXOPiLKkDcBNQBG4OiK2SboC\n6I6IzcAGSe8DBoHngQsnqsIDbbOGpz0sY2aWraknokfEFmBL3bLLaqb/ssX1amhu5/4qF0q+XMbM\nLEvuBq3nHNk+PO1hGTOzbLkL97kLZwxP+1JIM7NsuQv3OQtnDk873M3MsuUu3OcumT087ccPmJll\ny106zj3m0OHpYlvuqm9mdlDkLh1nHN4xPO1hGTOzbLkL91oeljEzy5brdPSwjJlZtlyno4dlzMyy\n5TvcPSxjZpYp1+lYaPPjB8zMsuQ63P34ATOzbLkM98N4MZmQw93MLEsuw/39s28DoER5citiZjZF\n5TLc//udJ/LgxVcx57gjJ7sqZmZTUlPPc59qOk44lhO+8ueTXQ0zsykrlz13MzMbW1PhLmmNpO2S\neiRdkrH+E5IelHSfpFskLW19Vc3MrFnjhrukInAlcA6wElgnaWVdsbuBrog4Cbge+GKrK2pmZs1r\npue+GuiJiB0RMQBsAs6vLRARt0bEK+ns7cDi1lbTzMwORDPhvgjYWTPfmy5r5CLgx1krJK2X1C2p\nu6+vr/lampnZAWkm3LPuFIrMgtIfA13AP2Stj4iNEdEVEV2dnZ3N19LMzA5IM5dC9gJLauYXA7vq\nC0l6H/AZ4IyI6G9N9czM7PVopue+FVghabmkduACYHNtAUmrgK8BayNid+uraWZmB0IRmSMsIwtJ\n5wJfBorA1RHxeUlXAN0RsVnST4ATgafSb3kyItaOs80+4InXWe/5wJ7X+b1TjdsyNbktU5PbAksj\nYtxx7abCfaqR1B0RXZNdj1ZwW6Ymt2Vqclua5ztUzcymIYe7mdk0lNdw3zjZFWght2VqclumJrel\nSbkcczczs7HlteduZmZjyF24j/eEyski6WpJuyU9ULPsCEk3S3o0/To3XS5JX0nbcJ+kk2u+58K0\n/KOSLqxZfoqk+9Pv+Yo0Me8YlLRE0q2SHpK0TdJf5rgtMyTdKenetC3/IV2+XNIdab2+k96/gaSO\ndL4nXb+sZluXpsu3S/q9muUH9XiUVJR0t6Qb89wWSY+nx8A9krrTZbk7xtJ9zZF0vaSH0/83p02J\ntkREbj4k19n/GjgWaAfuBVZOdr3Suv02cDLwQM2yLwKXpNOXAP85nT6X5Pk7At4N3JEuPwLYkX6d\nm07PTdfdCZyWfs+PgXMmqB0LgZPT6UOBR0ieBprHtgiYnU63AXekdbwOuCBd/lXgz9PpvwC+mk5f\nAHwnnV6ZHmsdwPL0GCxOxvEIfAL4NnBjOp/LtgCPA/PrluXuGEv39U3gI+l0OzBnKrRlwg7CCfpH\nPA24qWb+UuDSya5XTX2WMTLctwML0+mFwPZ0+mvAuvpywDrgazXLv5YuWwg8XLN8RLkJbtMPgbPz\n3hbgEOBXwLtIbhwp1R9TwE3Aael0KS2n+uNsqNzBPh5JHv1xC3AWcGNat7y25XFGh3vujjHgMOAx\n0vOXU6kteRuWOdAnVE62BRHxFED6deilr43aMdby3ozlEyr9U34VSY83l21JhzHuAXYDN5P0Tl+I\niKG3q9fuf7jO6foXgXkceBsnypeBvwGq6fw88tuWAP63pLskrU+X5fEYOxboA/5HOlz2dUmzmAJt\nyVu4N/2EyimuUTsOdPmEkTQb+B7w7yPipbGKZiybMm2JiEpEvJOk17saOGGM/U/Ztkj6fWB3RNxV\nu3iM/U/ZtqTeGxEnk7wE6GOSfnuMslO5LSWS4dirImIVsI9kGKaRg9aWvIV7U0+onEKekbQQIP06\n9FC1Ru0Ya/nijOUTQlIbSbB/KyJuSBfnsi1DIuIF4DaScc45koaeiFq7/+E6p+sPB57jwNs4Ed4L\nrJX0OMkLc84i6cnnsS1ExK70627g+yS/ePN4jPUCvRFxRzp/PUnYT35bJmpMbYLGt0okJxqWs/+k\nz9smu1419VvGyDH3f2DkSZUvptPnMfKkyp3p8iNIxu/mpp/HgCPSdVvTskMnVc6doDYIuAb4ct3y\nPLalE5iTTs8Efgb8PvBdRp6E/It0+mOMPAl5XTr9NkaehNxBcgJyUo5H4Ez2n1DNXVuAWcChNdP/\nD1iTx2Ms3dfPgOPT6cvTdkx6Wyb0IJygf8hzSa7g+DXwmcmuT029riV5KuYgyW/bi0jGOG8BHk2/\nDv2wRPJe2l8D95O8f3ZoOx8GetLPn9Ys7wIeSL/nn6k7gdPCdpxO8mfffcA96efcnLblJJL3+96X\n7u+ydPmxJFcg9JCEY0e6fEY635OuP7ZmW59J67udmqsVJuN4ZGS4564taZ3vTT/bhvaVx2Ms3dc7\nge70OPsBSThPelt8h6qZ2TSUtzF3MzNrgsPdzGwacribmU1DDnczs2nI4W5mNg053M3MpiGHu5nZ\nNORwNzObhv4/HjPh007FUXYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x211a30280f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2114acdf400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 60000/60000 [2:10:21<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# 8.6\n",
    "config.learning_rate = 0.1\n",
    "config.store_path = \"../../models/CNN/MNIST/withBN/\"\n",
    "losses, train_acc, test_acc = train(is_using_BN = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TRAIN NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## RESTORE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00000011921\n"
     ]
    }
   ],
   "source": [
    "#sess = tf.InteractiveSession()\n",
    "#sess.close()\n",
    "\n",
    "correct = []\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    (x_eval, y_eval, is_training), _, accuracy, y_out, loss, saver_eval = build_graph(is_using_BN=True)\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    for i in range(0, 2000, 500):\n",
    "        saver_eval = tf.train.import_meta_graph('../../models/CNN/mnist/withBN/model.ckpt-50000.meta')\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        saver_eval.restore(sess, \"../../models/CNN/mnist/withBN/model.ckpt-50000\")\n",
    "\n",
    "        cor = sess.run([accuracy],\n",
    "                       feed_dict={x_eval: dataset.x_train[i:i + 500], \n",
    "                                  y_eval: dataset.y_train[i:i + 500], \n",
    "                                  is_training: False})\n",
    "        correct.append(cor[0])\n",
    "        \n",
    "print (sum(correct) / len(correct))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
